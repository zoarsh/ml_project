{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbbefba-cce0-4d0a-9d6d-ef3cc4a71be0",
   "metadata": {
    "id": "3bbbefba-cce0-4d0a-9d6d-ef3cc4a71be0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0b25e4-d15b-415f-8bc4-2443895d6dc3",
   "metadata": {
    "id": "cb0b25e4-d15b-415f-8bc4-2443895d6dc3"
   },
   "source": [
    "#### !pip install pandas\n",
    "import pandas as pd\n",
    "#!pip install numpy\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9f1606-01a0-482a-8fc7-0242b65d3b5d",
   "metadata": {
    "id": "7b9f1606-01a0-482a-8fc7-0242b65d3b5d"
   },
   "source": [
    "# DATA PREP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f3a9f8-e95e-49b2-a991-e70b574e9184",
   "metadata": {
    "id": "86f3a9f8-e95e-49b2-a991-e70b574e9184"
   },
   "source": [
    "###  0) SETUP / UTILITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15873f8-066e-4471-b650-38e61acb706f",
   "metadata": {
    "id": "a15873f8-066e-4471-b650-38e61acb706f"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Rationale: Create a small \"infrastructure\" for any notebook so you can:\n",
    "# - Save frozen snapshots (so you can always roll back or compare states).\n",
    "# - Print quick, consistent progress reports between steps.\n",
    "# - Compare schemas (column names/types) before vs after each step.\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time, json\n",
    "\n",
    "# Create an artifacts/ folder to store snapshots and small metadata files.\n",
    "ARTIFACTS = Path(\"artifacts\")\n",
    "ARTIFACTS.mkdir(exist_ok=True)\n",
    "\n",
    "def timestamp():\n",
    "    \"\"\"Return a compact timestamp string like '20250914-1345'.\"\"\"\n",
    "    return time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "def save_snapshot_df(df: pd.DataFrame, name: str) -> str:\n",
    "    \"\"\"\n",
    "    Rationale: Persist a frozen copy of the DataFrame at important milestones\n",
    "    so you can restore or audit later.\n",
    "    Action: Try saving as parquet (preferred). If parquet engine is missing,\n",
    "    fall back to CSV. Also save a tiny JSON with shape and dtypes.\n",
    "    Returns: base path (string) without extension.\n",
    "    \"\"\"\n",
    "    ts = timestamp()\n",
    "    base = ARTIFACTS / f\"{ts}_{name}\"\n",
    "    try:\n",
    "        # Prefer parquet: space-efficient and fast to load.\n",
    "        df.to_parquet(f\"{base}.parquet\", index=False)\n",
    "        stored = f\"{base}.parquet\"\n",
    "    except Exception:\n",
    "        # Fallback to CSV if parquet engine isn't available.\n",
    "        df.to_csv(f\"{base}.csv\", index=False)\n",
    "        stored = f\"{base}.csv\"\n",
    "\n",
    "    # Save a minimal schema/shape summary for quick reference.\n",
    "    schema = {\n",
    "        \"name\": name,\n",
    "        \"rows\": int(len(df)),\n",
    "        \"cols\": int(df.shape[1]),\n",
    "        \"dtypes\": {c: str(t) for c, t in df.dtypes.items()}\n",
    "    }\n",
    "    with open(f\"{base}_schema.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(schema, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"[Saved] {stored} + {base.name}_schema.json\")\n",
    "    return str(base)\n",
    "\n",
    "def quick_data_report(df: pd.DataFrame, title: str = \"Dataset Report\"):\n",
    "    \"\"\"\n",
    "    Rationale: A compact health check after each major step.\n",
    "    Action: Print shape, type distribution, top missing columns, duplicates,\n",
    "    and short numeric/categorical describes.\n",
    "    \"\"\"\n",
    "    print(f\"=== {title} ===\\n\")\n",
    "    print(f\"Shape: {df.shape[0]:,} rows x {df.shape[1]:,} cols\")\n",
    "\n",
    "    print(\"\\n-- Dtypes summary --\")\n",
    "    print(df.dtypes.value_counts())\n",
    "\n",
    "    na = df.isna().sum()\n",
    "    na = na[na > 0].sort_values(ascending=False)\n",
    "    print(\"\\n-- Missing (top 10) --\")\n",
    "    print(na.head(10))\n",
    "\n",
    "    print(\"\\n-- Duplicated rows --\")\n",
    "    print(df.duplicated().sum())\n",
    "\n",
    "    print(\"\\n-- Numeric describe (head) --\")\n",
    "    print(df.describe(include=[np.number]).T.head(8))\n",
    "\n",
    "    print(\"\\n-- Categorical describe (head) --\")\n",
    "    print(df.describe(include=['object','category']).T.head(8))\n",
    "\n",
    "def diff_schema(df_before: pd.DataFrame, df_after: pd.DataFrame, title: str = \"Schema Diff\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Rationale: See what changed (columns added/dropped or dtypes converted).\n",
    "    Action: Compare column → dtype mapping before vs after and print only changes.\n",
    "    \"\"\"\n",
    "    a = pd.Series({c: str(t) for c, t in df_before.dtypes.items()}, name=\"before\")\n",
    "    b = pd.Series({c: str(t) for c, t in df_after.dtypes.items()},  name=\"after\")\n",
    "    allc = sorted(set(a.index) | set(b.index))\n",
    "    diff = pd.DataFrame({\"before\": a.reindex(allc), \"after\": b.reindex(allc)})\n",
    "    diff[\"changed\"] = diff[\"before\"] != diff[\"after\"]\n",
    "    print(f\"=== {title} ===\")\n",
    "    print(diff[diff[\"changed\"]])\n",
    "    return diff\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e88335-f681-4c2d-a46c-987d86649e3e",
   "metadata": {
    "id": "a15873f8-066e-4471-b650-38e61acb706f"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Rationale: Create a small \"infrastructure\" for any notebook so you can:\n",
    "# - Save frozen snapshots (so you can always roll back or compare states).\n",
    "# - Print quick, consistent progress reports between steps.\n",
    "# - Compare schemas (column names/types) before vs after each step.\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time, json\n",
    "\n",
    "# Create an artifacts/ folder to store snapshots and small metadata files.\n",
    "ARTIFACTS = Path(\"artifacts\")\n",
    "ARTIFACTS.mkdir(exist_ok=True)\n",
    "\n",
    "def timestamp():\n",
    "    \"\"\"Return a compact timestamp string like '20250914-1345'.\"\"\"\n",
    "    return time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "def save_snapshot_df(df: pd.DataFrame, name: str) -> str:\n",
    "    \"\"\"\n",
    "    Rationale: Persist a frozen copy of the DataFrame at important milestones\n",
    "    so you can restore or audit later.\n",
    "    Action: Try saving as parquet (preferred). If parquet engine is missing,\n",
    "    fall back to CSV. Also save a tiny JSON with shape and dtypes.\n",
    "    Returns: base path (string) without extension.\n",
    "    \"\"\"\n",
    "    ts = timestamp()\n",
    "    base = ARTIFACTS / f\"{ts}_{name}\"\n",
    "    try:\n",
    "        # Prefer parquet: space-efficient and fast to load.\n",
    "        df.to_parquet(f\"{base}.parquet\", index=False)\n",
    "        stored = f\"{base}.parquet\"\n",
    "    except Exception:\n",
    "        # Fallback to CSV if parquet engine isn't available.\n",
    "        df.to_csv(f\"{base}.csv\", index=False)\n",
    "        stored = f\"{base}.csv\"\n",
    "\n",
    "    # Save a minimal schema/shape summary for quick reference.\n",
    "    schema = {\n",
    "        \"name\": name,\n",
    "        \"rows\": int(len(df)),\n",
    "        \"cols\": int(df.shape[1]),\n",
    "        \"dtypes\": {c: str(t) for c, t in df.dtypes.items()}\n",
    "    }\n",
    "    with open(f\"{base}_schema.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(schema, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"[Saved] {stored} + {base.name}_schema.json\")\n",
    "    return str(base)\n",
    "\n",
    "def quick_data_report(df: pd.DataFrame, title: str = \"Dataset Report\"):\n",
    "    \"\"\"\n",
    "    Rationale: A compact health check after each major step.\n",
    "    Action: Print shape, type distribution, top missing columns, duplicates,\n",
    "    and short numeric/categorical describes.\n",
    "    \"\"\"\n",
    "    print(f\"=== {title} ===\\n\")\n",
    "    print(f\"Shape: {df.shape[0]:,} rows x {df.shape[1]:,} cols\")\n",
    "\n",
    "    print(\"\\n-- Dtypes summary --\")\n",
    "    print(df.dtypes.value_counts())\n",
    "\n",
    "    na = df.isna().sum()\n",
    "    na = na[na > 0].sort_values(ascending=False)\n",
    "    print(\"\\n-- Missing (top 10) --\")\n",
    "    print(na.head(10))\n",
    "\n",
    "    print(\"\\n-- Duplicated rows --\")\n",
    "    print(df.duplicated().sum())\n",
    "\n",
    "    print(\"\\n-- Numeric describe (head) --\")\n",
    "    print(df.describe(include=[np.number]).T.head(8))\n",
    "\n",
    "    print(\"\\n-- Categorical describe (head) --\")\n",
    "    print(df.describe(include=['object','category']).T.head(8))\n",
    "\n",
    "def diff_schema(df_before: pd.DataFrame, df_after: pd.DataFrame, title: str = \"Schema Diff\") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Rationale: See what changed (columns added/dropped or dtypes converted).\n",
    "    Action: Compare column → dtype mapping before vs after and print only changes.\n",
    "    \"\"\"\n",
    "    a = pd.Series({c: str(t) for c, t in df_before.dtypes.items()}, name=\"before\")\n",
    "    b = pd.Series({c: str(t) for c, t in df_after.dtypes.items()},  name=\"after\")\n",
    "    allc = sorted(set(a.index) | set(b.index))\n",
    "    diff = pd.DataFrame({\"before\": a.reindex(allc), \"after\": b.reindex(allc)})\n",
    "    diff[\"changed\"] = diff[\"before\"] != diff[\"after\"]\n",
    "    print(f\"=== {title} ===\")\n",
    "    print(diff[diff[\"changed\"]])\n",
    "    return diff\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb88a43f-6786-4698-ab2a-5d08969d6233",
   "metadata": {
    "id": "eb88a43f-6786-4698-ab2a-5d08969d6233"
   },
   "source": [
    "### 1. Uploading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840e2cc0-0d81-4a7b-a775-92eeb5504e23",
   "metadata": {
    "id": "840e2cc0-0d81-4a7b-a775-92eeb5504e23",
    "outputId": "92097620-3ee8-4d1a-a3b4-7fb8699fd446"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: (32561, 8) (5677, 16)\n"
     ]
    }
   ],
   "source": [
    "# === 1) LOAD TWO CSVs (df1, df2) ============================================\n",
    "# Rationale: Keep each file in its own DataFrame so we can compare before merging.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df1 = pd.read_csv(\"income_1.csv\", index_col=0)\n",
    "df2 = pd.read_csv(\"income_v2.csv\", index_col=0)  # <- adjust name if needed\n",
    "print(\"Loaded:\", df1.shape, df2.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5941a9-b5d5-44f6-aee9-e11b61051738",
   "metadata": {
    "id": "9d5941a9-b5d5-44f6-aee9-e11b61051738"
   },
   "source": [
    "### 2) QUICK PER-FILE REPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bd7988-ae9d-4595-9f58-ce93aa5f6a63",
   "metadata": {
    "id": "a4bd7988-ae9d-4595-9f58-ce93aa5f6a63",
    "outputId": "210931bd-45bb-4055-8a1b-60eb086caff8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== df1: Initial ===\n",
      "Shape: (32561, 8)\n",
      "\n",
      "-- Info --\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 32561 entries, 0 to 32560\n",
      "Data columns (total 8 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   age             32561 non-null  int64 \n",
      " 1   workclass       32561 non-null  object\n",
      " 2   fnlwgt          32561 non-null  int64 \n",
      " 3   education       32561 non-null  object\n",
      " 4   education.num   32561 non-null  int64 \n",
      " 5   marital.status  32561 non-null  object\n",
      " 6   occupation      32561 non-null  object\n",
      " 7   relationship    32561 non-null  object\n",
      "dtypes: int64(3), object(5)\n",
      "memory usage: 2.2+ MB\n",
      "\n",
      "-- Dtypes summary --\n",
      "object    5\n",
      "int64     3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "-- Unique values per column (top 10) --\n",
      "fnlwgt            21648\n",
      "age                  73\n",
      "education            16\n",
      "education.num        16\n",
      "occupation           15\n",
      "workclass             9\n",
      "marital.status        7\n",
      "relationship          6\n",
      "dtype: int64\n",
      "\n",
      "-- Missing (top 10) --\n",
      "Series([], dtype: int64)\n",
      "\n",
      "-- Duplicated rows -- 90\n",
      "\n",
      "-- Numeric describe (head) --\n",
      "                 count           mean            std      min       25%  \\\n",
      "age            32561.0      38.581647      13.640433     17.0      28.0   \n",
      "fnlwgt         32561.0  189778.366512  105549.977697  12285.0  117827.0   \n",
      "education.num  32561.0      10.080679       2.572720      1.0       9.0   \n",
      "\n",
      "                    50%       75%        max  \n",
      "age                37.0      48.0       90.0  \n",
      "fnlwgt         178356.0  237051.0  1484705.0  \n",
      "education.num      10.0      12.0       16.0  \n",
      "\n",
      "-- Categorical describe (head) --\n",
      "                count unique                 top   freq\n",
      "workclass       32561      9             Private  22696\n",
      "education       32561     16             HS-grad  10501\n",
      "marital.status  32561      7  Married-civ-spouse  14976\n",
      "occupation      32561     15      Prof-specialty   4140\n",
      "relationship    32561      6             Husband  13193\n",
      "\n",
      "=== df2: Initial ===\n",
      "Shape: (5677, 16)\n",
      "\n",
      "-- Info --\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5677 entries, 0 to 5676\n",
      "Data columns (total 16 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   ID              5677 non-null   int64 \n",
      " 1   age             5677 non-null   int64 \n",
      " 2   workclass       5402 non-null   object\n",
      " 3   fnlwgt          5677 non-null   int64 \n",
      " 4   education       5677 non-null   object\n",
      " 5   education.num   5677 non-null   int64 \n",
      " 6   marital.status  5677 non-null   object\n",
      " 7   occupation      5402 non-null   object\n",
      " 8   hours.per.week  5677 non-null   int64 \n",
      " 9   native.country  5563 non-null   object\n",
      " 10  income          5677 non-null   object\n",
      " 11  relationship    5677 non-null   object\n",
      " 12  race            5677 non-null   object\n",
      " 13  sex             5677 non-null   int64 \n",
      " 14  capital.loss    5677 non-null   int64 \n",
      " 15  age_range       5677 non-null   object\n",
      "dtypes: int64(7), object(9)\n",
      "memory usage: 754.0+ KB\n",
      "\n",
      "-- Dtypes summary --\n",
      "object    9\n",
      "int64     7\n",
      "Name: count, dtype: int64\n",
      "\n",
      "-- Unique values per column (top 10) --\n",
      "ID                5677\n",
      "fnlwgt            5004\n",
      "capital.loss        92\n",
      "hours.per.week      76\n",
      "age                 69\n",
      "education           16\n",
      "education.num       16\n",
      "occupation          15\n",
      "native.country      14\n",
      "workclass            8\n",
      "dtype: int64\n",
      "\n",
      "-- Missing (top 10) --\n",
      "workclass         275\n",
      "occupation        275\n",
      "native.country    114\n",
      "dtype: int64\n",
      "\n",
      "-- Duplicated rows -- 0\n",
      "\n",
      "-- Numeric describe (head) --\n",
      "                 count          mean            std        min        25%  \\\n",
      "ID              5677.0  1.551759e+06    1638.953070  1548921.0  1550340.0   \n",
      "age             5677.0  4.192919e+01      13.315780       17.0       32.0   \n",
      "fnlwgt          5677.0  1.880665e+05  103317.758130    19302.0   117585.0   \n",
      "education.num   5677.0  1.076960e+01       2.651028        1.0        9.0   \n",
      "hours.per.week  5677.0  4.252035e+01      12.276866        1.0       40.0   \n",
      "sex             5677.0  2.575304e-01       0.437312        0.0        0.0   \n",
      "capital.loss    5677.0  5.007398e+02     851.119416        0.0        0.0   \n",
      "\n",
      "                      50%        75%        max  \n",
      "ID              1551759.0  1553178.0  1554597.0  \n",
      "age                  41.0       50.0       90.0  \n",
      "fnlwgt           176186.0   236990.0  1033222.0  \n",
      "education.num        10.0       13.0       16.0  \n",
      "hours.per.week       40.0       50.0       99.0  \n",
      "sex                   0.0        1.0        1.0  \n",
      "capital.loss          0.0     1485.0     4356.0  \n",
      "\n",
      "-- Categorical describe (head) --\n",
      "               count unique                 top  freq\n",
      "workclass       5402      7             Private  3708\n",
      "education       5677     16              HSgrad  1583\n",
      "marital.status  5677      7  Married-civ-spouse  3408\n",
      "occupation      5402     14       Profspecialty  1021\n",
      "native.country  5563     13        UnitedStates  5154\n",
      "income          5677      2               <=50K  2937\n",
      "relationship    5677      6             Husband  3001\n",
      "race            5677      5               White  4967\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Rationale: Understand each dataset separately (shape, dtypes, missing, duplicates).\n",
    "\n",
    "# === QUICK DATA REPORT (with info + unique counts) ==========================\n",
    "def quick_data_report(df, title=\"Report\"):\n",
    "    print(f\"\\n=== {title} ===\")\n",
    "\n",
    "    # Shape\n",
    "    print(\"Shape:\", df.shape)\n",
    "\n",
    "    # Info: non-null counts and dtypes\n",
    "    print(\"\\n-- Info --\")\n",
    "    df.info()\n",
    "\n",
    "    # Dtypes summary\n",
    "    print(\"\\n-- Dtypes summary --\")\n",
    "    print(df.dtypes.value_counts())\n",
    "\n",
    "    # Unique counts\n",
    "    print(\"\\n-- Unique values per column (top 10) --\")\n",
    "    nunique = df.nunique(dropna=False).sort_values(ascending=False)\n",
    "    print(nunique.head(10))\n",
    "\n",
    "    # Missing values\n",
    "    na = df.isna().sum()\n",
    "    na = na[na > 0].sort_values(ascending=False)\n",
    "    print(\"\\n-- Missing (top 10) --\")\n",
    "    print(na.head(10))\n",
    "\n",
    "    # Duplicates\n",
    "    print(\"\\n-- Duplicated rows --\", df.duplicated().sum())\n",
    "\n",
    "    # Numeric stats\n",
    "    print(\"\\n-- Numeric describe (head) --\")\n",
    "    print(df.describe(include=[np.number]).T.head(8))\n",
    "\n",
    "    # Categorical stats\n",
    "    print(\"\\n-- Categorical describe (head) --\")\n",
    "    print(df.describe(include=['object','category']).T.head(8))\n",
    "\n",
    "# Run for both datasets\n",
    "quick_data_report(df1, \"df1: Initial\")\n",
    "quick_data_report(df2, \"df2: Initial\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902acb61-48a6-49bf-bd1a-b9ef57b70915",
   "metadata": {
    "id": "902acb61-48a6-49bf-bd1a-b9ef57b70915",
    "outputId": "6d6a5221-6f95-4b2c-acf8-bc29b9301688"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### df1: Initial"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>dtype</th>\n",
       "      <th>non_null</th>\n",
       "      <th>missing</th>\n",
       "      <th>unique</th>\n",
       "      <th>%missing</th>\n",
       "      <th>sample_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>age</td>\n",
       "      <td>int64</td>\n",
       "      <td>32561</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90, 82, 66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>workclass</td>\n",
       "      <td>object</td>\n",
       "      <td>32561</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>?, Private, State-gov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fnlwgt</td>\n",
       "      <td>int64</td>\n",
       "      <td>32561</td>\n",
       "      <td>0</td>\n",
       "      <td>21648</td>\n",
       "      <td>0.0</td>\n",
       "      <td>77053, 132870, 186061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>education</td>\n",
       "      <td>object</td>\n",
       "      <td>32561</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>HS-grad, Some-college, 7th-8th</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>education.num</td>\n",
       "      <td>int64</td>\n",
       "      <td>32561</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9, 10, 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>marital.status</td>\n",
       "      <td>object</td>\n",
       "      <td>32561</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Widowed, Divorced, Separated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>occupation</td>\n",
       "      <td>object</td>\n",
       "      <td>32561</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0.0</td>\n",
       "      <td>?, Exec-managerial, Machine-op-inspct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>relationship</td>\n",
       "      <td>object</td>\n",
       "      <td>32561</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Not-in-family, Unmarried, Own-child</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           column   dtype  non_null  missing  unique  %missing  \\\n",
       "0             age   int64     32561        0      73       0.0   \n",
       "1       workclass  object     32561        0       9       0.0   \n",
       "2          fnlwgt   int64     32561        0   21648       0.0   \n",
       "3       education  object     32561        0      16       0.0   \n",
       "4   education.num   int64     32561        0      16       0.0   \n",
       "5  marital.status  object     32561        0       7       0.0   \n",
       "6      occupation  object     32561        0      15       0.0   \n",
       "7    relationship  object     32561        0       6       0.0   \n",
       "\n",
       "                           sample_values  \n",
       "0                             90, 82, 66  \n",
       "1                  ?, Private, State-gov  \n",
       "2                  77053, 132870, 186061  \n",
       "3         HS-grad, Some-college, 7th-8th  \n",
       "4                               9, 10, 4  \n",
       "5           Widowed, Divorced, Separated  \n",
       "6  ?, Exec-managerial, Machine-op-inspct  \n",
       "7    Not-in-family, Unmarried, Own-child  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### df2: Initial"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>dtype</th>\n",
       "      <th>non_null</th>\n",
       "      <th>missing</th>\n",
       "      <th>unique</th>\n",
       "      <th>%missing</th>\n",
       "      <th>sample_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>workclass</td>\n",
       "      <td>object</td>\n",
       "      <td>5402</td>\n",
       "      <td>275</td>\n",
       "      <td>8</td>\n",
       "      <td>4.84</td>\n",
       "      <td>Private, Stategov, Federalgov</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>occupation</td>\n",
       "      <td>object</td>\n",
       "      <td>5402</td>\n",
       "      <td>275</td>\n",
       "      <td>15</td>\n",
       "      <td>4.84</td>\n",
       "      <td>Execmanagerial, Machineopinspct, Profspecialty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>native.country</td>\n",
       "      <td>object</td>\n",
       "      <td>5563</td>\n",
       "      <td>114</td>\n",
       "      <td>14</td>\n",
       "      <td>2.01</td>\n",
       "      <td>UnitedStates, North America, Asia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>age</td>\n",
       "      <td>int64</td>\n",
       "      <td>5677</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>0.00</td>\n",
       "      <td>90, 82, 66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fnlwgt</td>\n",
       "      <td>int64</td>\n",
       "      <td>5677</td>\n",
       "      <td>0</td>\n",
       "      <td>5004</td>\n",
       "      <td>0.00</td>\n",
       "      <td>77053, 132870, 186061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>education</td>\n",
       "      <td>object</td>\n",
       "      <td>5677</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>HSgrad, Somecollege, 7th8th</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>education.num</td>\n",
       "      <td>int64</td>\n",
       "      <td>5677</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9, 10, 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID</td>\n",
       "      <td>int64</td>\n",
       "      <td>5677</td>\n",
       "      <td>0</td>\n",
       "      <td>5677</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1548921, 1548922, 1548923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>marital.status</td>\n",
       "      <td>object</td>\n",
       "      <td>5677</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Widowed, Divorced, Separated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hours.per.week</td>\n",
       "      <td>int64</td>\n",
       "      <td>5677</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>0.00</td>\n",
       "      <td>40, 18, 45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           column   dtype  non_null  missing  unique  %missing  \\\n",
       "2       workclass  object      5402      275       8      4.84   \n",
       "7      occupation  object      5402      275      15      4.84   \n",
       "9  native.country  object      5563      114      14      2.01   \n",
       "1             age   int64      5677        0      69      0.00   \n",
       "3          fnlwgt   int64      5677        0    5004      0.00   \n",
       "4       education  object      5677        0      16      0.00   \n",
       "5   education.num   int64      5677        0      16      0.00   \n",
       "0              ID   int64      5677        0    5677      0.00   \n",
       "6  marital.status  object      5677        0       7      0.00   \n",
       "8  hours.per.week   int64      5677        0      76      0.00   \n",
       "\n",
       "                                    sample_values  \n",
       "2                   Private, Stategov, Federalgov  \n",
       "7  Execmanagerial, Machineopinspct, Profspecialty  \n",
       "9               UnitedStates, North America, Asia  \n",
       "1                                      90, 82, 66  \n",
       "3                           77053, 132870, 186061  \n",
       "4                     HSgrad, Somecollege, 7th8th  \n",
       "5                                        9, 10, 4  \n",
       "0                       1548921, 1548922, 1548923  \n",
       "6                    Widowed, Divorced, Separated  \n",
       "8                                      40, 18, 45  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === ULTRA DATA REPORT ======================================================\n",
    "# Rationale: Build one compact summary table with key facts per column.\n",
    "# Includes: column name | dtype | non-null | missing | %missing | unique | sample values.\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "def ultra_data_report(df, title=\"Dataset Report\"):\n",
    "    \"\"\"Return a DataFrame summarizing each column in df.\"\"\"\n",
    "    report = pd.DataFrame({\n",
    "        \"dtype\": df.dtypes.astype(str),\n",
    "        \"non_null\": df.notna().sum(),\n",
    "        \"missing\": df.isna().sum(),\n",
    "        \"unique\": df.nunique(dropna=False)\n",
    "    })\n",
    "    report[\"%missing\"] = (report[\"missing\"] / len(df) * 100).round(2)\n",
    "\n",
    "    # Add sample values (up to 3 unique values as a preview)\n",
    "    samples = {}\n",
    "    for col in df.columns:\n",
    "        vals = df[col].dropna().unique()[:3]\n",
    "        samples[col] = \", \".join(map(str, vals))\n",
    "    report[\"sample_values\"] = pd.Series(samples)\n",
    "\n",
    "    report = report.reset_index().rename(columns={\"index\": \"column\"})\n",
    "    report = report.sort_values(\"%missing\", ascending=False)\n",
    "    return report\n",
    "\n",
    "# === RUN ULTRA REPORTS FOR df1 and df2 ======================================\n",
    "ultra_df1 = ultra_data_report(df1, \"df1: Initial\")\n",
    "ultra_df2 = ultra_data_report(df2, \"df2: Initial\")\n",
    "\n",
    "# Display with Markdown headings directly above each table\n",
    "display(Markdown(\"### df1: Initial\"))\n",
    "display(ultra_df1.head(10))\n",
    "\n",
    "display(Markdown(\"### df2: Initial\"))\n",
    "display(ultra_df2.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863e6292-cb52-4570-9f1a-9871b3ad15ad",
   "metadata": {
    "id": "863e6292-cb52-4570-9f1a-9871b3ad15ad",
    "outputId": "7d89415f-491c-4a51-eb9f-830367036c47"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common columns: ['age', 'education', 'education.num', 'fnlwgt', 'marital.status', 'occupation', 'relationship', 'workclass']\n",
      "\n",
      "Columns only in df1: []\n",
      "\n",
      "Columns only in df2: ['ID', 'hours.per.week', 'native.country', 'income', 'race', 'sex', 'capital.loss', 'age_range']\n",
      "\n",
      "Dtype mismatches among common columns:\n",
      " Empty DataFrame\n",
      "Columns: [df1, df2]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# === 3) SCHEMA COMPARISON (COLUMNS & DTYPES) — KEEP AS IS ====================\n",
    "# Purpose: See which columns are shared and whether dtypes align.\n",
    "# Action: No change needed.\n",
    "\n",
    "c1 = pd.Series(df1.dtypes.astype(str), name=\"df1\")\n",
    "c2 = pd.Series(df2.dtypes.astype(str), name=\"df2\")\n",
    "all_cols = sorted(set(c1.index) | set(c2.index))\n",
    "schema = pd.concat([c1.reindex(all_cols), c2.reindex(all_cols)], axis=1)\n",
    "\n",
    "same_cols = [c for c in all_cols if (c in df1.columns) and (c in df2.columns)]\n",
    "only_df1  = [c for c in df1.columns if c not in df2.columns]\n",
    "only_df2  = [c for c in df2.columns if c not in df1.columns]\n",
    "\n",
    "print(\"Common columns:\", same_cols)\n",
    "print(\"\\nColumns only in df1:\", only_df1)\n",
    "print(\"\\nColumns only in df2:\", only_df2)\n",
    "\n",
    "mismatch = schema.loc[same_cols]\n",
    "mismatch = mismatch[mismatch[\"df1\"] != mismatch[\"df2\"]]\n",
    "print(\"\\nDtype mismatches among common columns:\\n\", mismatch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05ba3a1-af92-4445-b327-36b520b12c16",
   "metadata": {
    "id": "e05ba3a1-af92-4445-b327-36b520b12c16"
   },
   "source": [
    "###  4) CHECK MERGE KEYS QUALITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabf3b45-a4dc-4255-a7eb-513821b18988",
   "metadata": {
    "id": "eabf3b45-a4dc-4255-a7eb-513821b18988",
    "outputId": "91bfcc05-4c5a-4b53-fb63-508e11dd5cb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing in keys (df1):\n",
      "age              0\n",
      "education.num    0\n",
      "fnlwgt           0\n",
      "dtype: int64\n",
      "\n",
      "Missing in keys (df2):\n",
      "age              0\n",
      "education.num    0\n",
      "fnlwgt           0\n",
      "dtype: int64\n",
      "\n",
      "Duplicate key rows - df1: 963, df2: 31\n",
      "Distinct key combos: df1=31598, df2=5646, overlap=5646\n"
     ]
    }
   ],
   "source": [
    "# === 4) QUICK KEY QUALITY CHECK — UPDATE KEYS ================================\n",
    "# Purpose: Sanity-check missing/duplicates/overlap for the chosen key.\n",
    "# Change: Use the better key we found: ['age','education.num','fnlwgt'].\n",
    "\n",
    "merge_keys = ['age', 'education.num', 'fnlwgt']  # CHOSEN KEY\n",
    "\n",
    "print(\"\\nMissing in keys (df1):\")\n",
    "print(df1[merge_keys].isna().sum())\n",
    "print(\"\\nMissing in keys (df2):\")\n",
    "print(df2[merge_keys].isna().sum())\n",
    "\n",
    "dup1 = df1.duplicated(subset=merge_keys).sum()\n",
    "dup2 = df2.duplicated(subset=merge_keys).sum()\n",
    "print(f\"\\nDuplicate key rows - df1: {dup1}, df2: {dup2}\")\n",
    "\n",
    "k1 = df1[merge_keys].drop_duplicates()\n",
    "k2 = df2[merge_keys].drop_duplicates()\n",
    "common_keys = len(pd.merge(k1, k2, on=merge_keys, how='inner'))\n",
    "print(f\"Distinct key combos: df1={len(k1)}, df2={len(k2)}, overlap={common_keys}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23afa3bb-f29b-4c3a-aa16-8caa8e2f5893",
   "metadata": {
    "id": "23afa3bb-f29b-4c3a-aa16-8caa8e2f5893",
    "outputId": "33c77e3f-ec7a-4f02-c8cf-6b749863db32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Keys=['fnlwgt']\n",
      "Missing df1/df2: 0 / 0\n",
      "Distinct combos df1/df2: 21648 / 5004 | overlap: 5004\n",
      "Duplicate rows by keys df1/df2: 10913 / 673\n",
      "\n",
      "Keys=['age', 'fnlwgt']\n",
      "Missing df1/df2: 0 / 0\n",
      "Distinct combos df1/df2: 29147 / 5514 | overlap: 5514\n",
      "Duplicate rows by keys df1/df2: 3414 / 163\n",
      "\n",
      "Keys=['fnlwgt', 'education.num']\n",
      "Missing df1/df2: 0 / 0\n",
      "Distinct combos df1/df2: 28855 / 5517 | overlap: 5517\n",
      "Duplicate rows by keys df1/df2: 3706 / 160\n",
      "\n",
      "Keys=['age', 'education.num', 'fnlwgt']\n",
      "Missing df1/df2: 0 / 0\n",
      "Distinct combos df1/df2: 31598 / 5646 | overlap: 5646\n",
      "Duplicate rows by keys df1/df2: 963 / 31\n"
     ]
    }
   ],
   "source": [
    "# === 4b) TRY A FEW SIMPLE KEY CANDIDATES — OPTIONAL (KEEP IF USEFUL) ========\n",
    "# Purpose: Compare a few candidates quickly. You can keep or skip once decided.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "candidates = [\n",
    "    ['fnlwgt'],\n",
    "    ['age','fnlwgt'],\n",
    "    ['fnlwgt','education.num'],\n",
    "    ['age','education.num','fnlwgt']  # this was the best by your output\n",
    "]\n",
    "\n",
    "def check_key(df1, df2, keys):\n",
    "    miss1 = df1[keys].isna().any(axis=1).sum()\n",
    "    miss2 = df2[keys].isna().any(axis=1).sum()\n",
    "    u1 = df1[keys].dropna().drop_duplicates().shape[0]\n",
    "    u2 = df2[keys].dropna().drop_duplicates().shape[0]\n",
    "    dup1 = df1.dropna(subset=keys).duplicated(subset=keys).sum()\n",
    "    dup2 = df2.dropna(subset=keys).duplicated(subset=keys).sum()\n",
    "    k1 = df1[keys].dropna().drop_duplicates()\n",
    "    k2 = df2[keys].dropna().drop_duplicates()\n",
    "    overlap = pd.merge(k1, k2, on=keys, how='inner').shape[0]\n",
    "    print(f\"\\nKeys={keys}\")\n",
    "    print(f\"Missing df1/df2: {miss1} / {miss2}\")\n",
    "    print(f\"Distinct combos df1/df2: {u1} / {u2} | overlap: {overlap}\")\n",
    "    print(f\"Duplicate rows by keys df1/df2: {dup1} / {dup2}\")\n",
    "\n",
    "for keys in candidates:\n",
    "    check_key(df1, df2, keys)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174db98e-1c09-4faa-af48-b73f3e2a3cd2",
   "metadata": {
    "id": "174db98e-1c09-4faa-af48-b73f3e2a3cd2"
   },
   "source": [
    "### 5) KEY STANDARDIZATION & ROW FILTERING — UPDATE KEYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b631be74-7d4f-49e3-bf49-28ef34cbc41f",
   "metadata": {
    "id": "b631be74-7d4f-49e3-bf49-28ef34cbc41f",
    "outputId": "276064ca-4052-40e7-a676-8d40e5e1f665"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After dropping missing-key rows: (32561, 8) (5677, 16)\n",
      "After simple de-dup per key: (31598, 8) (5646, 16)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Purpose: Clean string keys (if any), then drop rows with missing keys.\n",
    "# Change: Use the same chosen keys here.\n",
    "\n",
    "keys = ['age', 'education.num', 'fnlwgt']  # must match the chosen key above\n",
    "\n",
    "def clean_str_col(s):\n",
    "    # Only applied if dtype is object/string; safe no-op for numeric columns\n",
    "    if s.dtype == 'object' or pd.api.types.is_string_dtype(s):\n",
    "        s = (s.astype('string')\n",
    "               .str.strip()\n",
    "               .str.replace('-', '', regex=False)\n",
    "               .str.replace(' ', '', regex=False)\n",
    "               .replace({'': pd.NA, 'nan': pd.NA, 'None': pd.NA}))\n",
    "    return s\n",
    "\n",
    "for c in keys:\n",
    "    if c in df1.columns:\n",
    "        df1[c] = clean_str_col(df1[c])\n",
    "    if c in df2.columns:\n",
    "        df2[c] = clean_str_col(df2[c])\n",
    "\n",
    "# Drop rows lacking any part of the key (inner join requires complete keys anyway)\n",
    "df1_keys_ok = df1.dropna(subset=keys)\n",
    "df2_keys_ok = df2.dropna(subset=keys)\n",
    "print(\"After dropping missing-key rows:\", df1_keys_ok.shape, df2_keys_ok.shape)\n",
    "\n",
    "# (Optional) If you want to reduce many-to-many risk: deduplicate per key combo\n",
    "df1_keys_ok = df1_keys_ok.drop_duplicates(subset=keys, keep='first')\n",
    "df2_keys_ok = df2_keys_ok.drop_duplicates(subset=keys, keep='first')\n",
    "print(\"After simple de-dup per key:\", df1_keys_ok.shape, df2_keys_ok.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f7da90-992b-4e57-ae27-7bf8e9c2993f",
   "metadata": {
    "id": "13f7da90-992b-4e57-ae27-7bf8e9c2993f"
   },
   "source": [
    "###  6) MERGE — UPDATE TO USE THE CLEANED DFS & CHOSEN KEYS\n",
    "Inner Join                                                                                                                        \n",
    "This type of merge returns only the rows where there is a match between the two dataframes.<br>                                      \n",
    "Outer Join      <br>                                                                                                              This type of merge returns all rows from both dataframes, and fills in NaN where there are no matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ae7370-4a5f-4f97-aaf1-a9e7d0cace9d",
   "metadata": {
    "id": "71ae7370-4a5f-4f97-aaf1-a9e7d0cace9d",
    "outputId": "a6cf3f6f-601a-4929-fcf0-0e467a07ff2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged shape: (5646, 21)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass_x</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education_x</th>\n",
       "      <th>education.num</th>\n",
       "      <th>marital.status_x</th>\n",
       "      <th>occupation_x</th>\n",
       "      <th>relationship_x</th>\n",
       "      <th>ID</th>\n",
       "      <th>workclass_y</th>\n",
       "      <th>...</th>\n",
       "      <th>marital.status_y</th>\n",
       "      <th>occupation_y</th>\n",
       "      <th>hours.per.week</th>\n",
       "      <th>native.country</th>\n",
       "      <th>income</th>\n",
       "      <th>relationship_y</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital.loss</th>\n",
       "      <th>age_range</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>90</td>\n",
       "      <td>?</td>\n",
       "      <td>77053</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>?</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>1548921</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40</td>\n",
       "      <td>UnitedStates</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>Notinfamily</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>4356</td>\n",
       "      <td>&gt;70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>82</td>\n",
       "      <td>Private</td>\n",
       "      <td>132870</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>1548922</td>\n",
       "      <td>Private</td>\n",
       "      <td>...</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>Execmanagerial</td>\n",
       "      <td>18</td>\n",
       "      <td>UnitedStates</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>Notinfamily</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>4356</td>\n",
       "      <td>&gt;70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>66</td>\n",
       "      <td>?</td>\n",
       "      <td>186061</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>?</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>1548923</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Widowed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>40</td>\n",
       "      <td>UnitedStates</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>Black</td>\n",
       "      <td>1</td>\n",
       "      <td>4356</td>\n",
       "      <td>45-69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>Private</td>\n",
       "      <td>140359</td>\n",
       "      <td>7th-8th</td>\n",
       "      <td>4</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Machine-op-inspct</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>1548924</td>\n",
       "      <td>Private</td>\n",
       "      <td>...</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Machineopinspct</td>\n",
       "      <td>40</td>\n",
       "      <td>UnitedStates</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>Unmarried</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>3900</td>\n",
       "      <td>45-69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>41</td>\n",
       "      <td>Private</td>\n",
       "      <td>264663</td>\n",
       "      <td>Some-college</td>\n",
       "      <td>10</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Own-child</td>\n",
       "      <td>1548925</td>\n",
       "      <td>Private</td>\n",
       "      <td>...</td>\n",
       "      <td>Separated</td>\n",
       "      <td>Profspecialty</td>\n",
       "      <td>40</td>\n",
       "      <td>UnitedStates</td>\n",
       "      <td>&lt;=50K</td>\n",
       "      <td>Ownchild</td>\n",
       "      <td>White</td>\n",
       "      <td>1</td>\n",
       "      <td>3900</td>\n",
       "      <td>35-44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age workclass_x  fnlwgt   education_x  education.num marital.status_x  \\\n",
       "0   90           ?   77053       HS-grad              9          Widowed   \n",
       "1   82     Private  132870       HS-grad              9          Widowed   \n",
       "2   66           ?  186061  Some-college             10          Widowed   \n",
       "3   54     Private  140359       7th-8th              4         Divorced   \n",
       "4   41     Private  264663  Some-college             10        Separated   \n",
       "\n",
       "        occupation_x relationship_x       ID workclass_y  ...  \\\n",
       "0                  ?  Not-in-family  1548921         NaN  ...   \n",
       "1    Exec-managerial  Not-in-family  1548922     Private  ...   \n",
       "2                  ?      Unmarried  1548923         NaN  ...   \n",
       "3  Machine-op-inspct      Unmarried  1548924     Private  ...   \n",
       "4     Prof-specialty      Own-child  1548925     Private  ...   \n",
       "\n",
       "  marital.status_y     occupation_y hours.per.week  native.country income  \\\n",
       "0          Widowed              NaN             40    UnitedStates  <=50K   \n",
       "1          Widowed   Execmanagerial             18    UnitedStates  <=50K   \n",
       "2          Widowed              NaN             40    UnitedStates  <=50K   \n",
       "3         Divorced  Machineopinspct             40    UnitedStates  <=50K   \n",
       "4        Separated    Profspecialty             40    UnitedStates  <=50K   \n",
       "\n",
       "  relationship_y   race sex  capital.loss  age_range  \n",
       "0    Notinfamily  White   1          4356        >70  \n",
       "1    Notinfamily  White   1          4356        >70  \n",
       "2      Unmarried  Black   1          4356      45-69  \n",
       "3      Unmarried  White   1          3900      45-69  \n",
       "4       Ownchild  White   1          3900      35-44  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Purpose: Perform the merge with the chosen, cleaned key set.\n",
    "# Change: Merge df1_keys_ok with df2_keys_ok on ['age','education.num','fnlwgt'].\n",
    "\n",
    "merged_df = pd.merge(\n",
    "    df1_keys_ok, df2_keys_ok,\n",
    "    on=['age', 'education.num', 'fnlwgt'],\n",
    "    how='inner',\n",
    "    suffixes=('_x','_y')  # we'll resolve pairs later if needed\n",
    ")\n",
    "\n",
    "print(\"Merged shape:\", merged_df.shape)\n",
    "merged_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b52b5b2-e043-4631-9994-7ba1c711c18d",
   "metadata": {
    "id": "8b52b5b2-e043-4631-9994-7ba1c711c18d"
   },
   "source": [
    "###  7) SAVE RAW SNAPSHOT & PREP WORKING COPY — KEEP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945aa0c2-fe45-462e-b6f0-03f27eedbaa2",
   "metadata": {
    "id": "945aa0c2-fe45-462e-b6f0-03f27eedbaa2",
    "outputId": "0e95bb88-c809-47d2-c4aa-48cc9a3d50bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved] artifacts\\20250914-171233_raw_loaded.csv + 20250914-171233_raw_loaded_schema.json\n",
      "=== Initial Report (raw merged) ===\n",
      "\n",
      "Rows: 5,646, Columns: 21\n",
      "\n",
      "--- Dtypes summary ---\n",
      "object    14\n",
      "int64      7\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Unique values (Top 10) ---\n",
      "ID                5646\n",
      "fnlwgt            5004\n",
      "capital.loss        92\n",
      "hours.per.week      76\n",
      "age                 69\n",
      "education_x         16\n",
      "education.num       16\n",
      "education_y         16\n",
      "occupation_y        15\n",
      "occupation_x        15\n",
      "dtype: int64\n",
      "\n",
      "--- Missing values (Top 10) ---\n",
      "workclass_y       274\n",
      "occupation_y      274\n",
      "native.country    114\n",
      "dtype: int64\n",
      "\n",
      "--- Duplicated rows ---\n",
      "Total duplicated rows: 0\n",
      "\n",
      "--- Descriptive statistics (numeric, Top 10) ---\n",
      "                 count          mean            std        min         25%  \\\n",
      "age             5646.0  4.194793e+01      13.326337       17.0       32.00   \n",
      "fnlwgt          5646.0  1.880733e+05  103473.567902    19302.0   117478.75   \n",
      "education.num   5646.0  1.076922e+01       2.654484        1.0        9.00   \n",
      "ID              5646.0  1.551757e+06    1640.265137  1548921.0  1550334.25   \n",
      "hours.per.week  5646.0  4.251931e+01      12.286565        1.0       40.00   \n",
      "sex             5646.0  2.582359e-01       0.437703        0.0        0.00   \n",
      "capital.loss    5646.0  5.028277e+02     852.307416        0.0        0.00   \n",
      "\n",
      "                      50%         75%        max  \n",
      "age                  41.0       51.00       90.0  \n",
      "fnlwgt           176255.0   237311.00  1033222.0  \n",
      "education.num        10.0       13.00       16.0  \n",
      "ID              1551754.5  1553178.75  1554597.0  \n",
      "hours.per.week       40.0       50.00       99.0  \n",
      "sex                   0.0        1.00        1.0  \n",
      "capital.loss          0.0     1485.00     4356.0  \n",
      "\n",
      "--- Descriptive statistics (categorical/text, Top 10) ---\n",
      "                 count unique                 top  freq\n",
      "workclass_x       5646      8             Private  3683\n",
      "education_x       5646     16             HS-grad  1573\n",
      "marital.status_x  5646      7  Married-civ-spouse  3388\n",
      "occupation_x      5646     15      Prof-specialty  1019\n",
      "relationship_x    5646      6             Husband  2981\n",
      "workclass_y       5372      7             Private  3683\n",
      "education_y       5646     16              HSgrad  1573\n",
      "marital.status_y  5646      7  Married-civ-spouse  3388\n",
      "occupation_y      5372     14       Profspecialty  1019\n",
      "native.country    5532     13        UnitedStates  5126\n",
      "[Saved] Ultra Report → 20250914-171233_ultra_report_raw_merged.csv and 20250914-171233_ultra_report_raw_merged.xlsx\n"
     ]
    }
   ],
   "source": [
    "# === SAVE RAW SNAPSHOT + CREATE WORKING COPY + EXPORT ULTRA REPORT ===========\n",
    "# Purpose:\n",
    "# 1) Persist a frozen copy of the merged dataset to disk (parquet if possible, else CSV).\n",
    "# 2) Keep an in-memory immutable reference (df_raw) you never touch.\n",
    "# 3) Work only on df (all cleaning happens here).\n",
    "# 4) Generate a compact, tabular \"Ultra Report\" of columns and save it to CSV/Excel.\n",
    "\n",
    "import time, json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# --- Safety: ensure merge was executed ---\n",
    "assert 'merged_df' in globals(), \"merged_df is not defined. Run the merge cell first.\"\n",
    "\n",
    "# --- Ensure artifacts folder & timestamp utility exist ---\n",
    "ARTIFACTS = Path(\"artifacts\"); ARTIFACTS.mkdir(exist_ok=True)\n",
    "def timestamp(): return time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# --- Snapshot helper (always available here) ---\n",
    "def save_snapshot_df(df: pd.DataFrame, name: str) -> str:\n",
    "    base = ARTIFACTS / f\"{timestamp()}_{name}\"\n",
    "    try:\n",
    "        df.to_parquet(f\"{base}.parquet\", index=False)\n",
    "        stored = f\"{base}.parquet\"\n",
    "    except Exception:\n",
    "        df.to_csv(f\"{base}.csv\", index=False)\n",
    "        stored = f\"{base}.csv\"\n",
    "    schema = {\n",
    "        \"name\": name,\n",
    "        \"rows\": int(len(df)),\n",
    "        \"cols\": int(df.shape[1]),\n",
    "        \"dtypes\": {c: str(t) for c, t in df.dtypes.items()}\n",
    "    }\n",
    "    with open(f\"{base}_schema.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(schema, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"[Saved] {stored} + {base.name}_schema.json\")\n",
    "    return str(base)\n",
    "\n",
    "# --- ALWAYS (re)define quick_data_report with title/top_n --------------------\n",
    "try:\n",
    "    del quick_data_report  # remove old signature if it exists\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "def quick_data_report(df, title=\"Dataset Report\", top_n=10):\n",
    "    print(f\"=== {title} ===\\n\")\n",
    "    print(f\"Rows: {df.shape[0]:,}, Columns: {df.shape[1]:,}\")\n",
    "\n",
    "    print(\"\\n--- Dtypes summary ---\")\n",
    "    print(df.dtypes.value_counts())\n",
    "\n",
    "    nunique = df.nunique(dropna=False).sort_values(ascending=False)\n",
    "    print(f\"\\n--- Unique values (Top {top_n}) ---\")\n",
    "    print(nunique.head(top_n))\n",
    "\n",
    "    na_counts = df.isna().sum()\n",
    "    na_counts = na_counts[na_counts > 0].sort_values(ascending=False)\n",
    "    print(f\"\\n--- Missing values (Top {top_n}) ---\")\n",
    "    print(na_counts.head(top_n))\n",
    "\n",
    "    print(\"\\n--- Duplicated rows ---\")\n",
    "    print(\"Total duplicated rows:\", int(df.duplicated().sum()))\n",
    "\n",
    "    num_desc = df.describe(include='number').T\n",
    "    if not num_desc.empty:\n",
    "        print(f\"\\n--- Descriptive statistics (numeric, Top {top_n}) ---\")\n",
    "        print(num_desc.head(top_n))\n",
    "\n",
    "    cat_desc = df.describe(include=['object', 'string', 'category']).T\n",
    "    if not cat_desc.empty:\n",
    "        print(f\"\\n--- Descriptive statistics (categorical/text, Top {top_n}) ---\")\n",
    "        print(cat_desc.head(top_n))\n",
    "\n",
    "# --- 1) Save frozen snapshot of merged_df ---\n",
    "RAW_SNAPSHOT_PATH = save_snapshot_df(merged_df, \"raw_loaded\")\n",
    "\n",
    "# --- 2) Create immutable and working copies ---\n",
    "df_raw = merged_df.copy(deep=True)   # do not modify\n",
    "df     = merged_df.copy(deep=True)   # all cleaning will use 'df'\n",
    "\n",
    "# --- 3) Quick sanity report on the starting point ---\n",
    "quick_data_report(df, title=\"Initial Report (raw merged)\", top_n=10)\n",
    "\n",
    "# --- 4) Ultra Report builder + export ---------------------------------------\n",
    "def build_ultra_report(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    rep = pd.DataFrame({\n",
    "        \"dtype\": df.dtypes.astype(str),\n",
    "        \"non_null\": df.notna().sum(),\n",
    "        \"missing\": df.isna().sum(),\n",
    "        \"unique\": df.nunique(dropna=False)\n",
    "    })\n",
    "    rep[\"%missing\"] = (rep[\"missing\"] / len(df) * 100).round(2)\n",
    "    samples = {}\n",
    "    for col in df.columns:\n",
    "        vals = df[col].dropna().unique()[:3]\n",
    "        samples[col] = \", \".join(map(str, vals))\n",
    "    rep[\"sample_values\"] = pd.Series(samples)\n",
    "    rep = rep.reset_index().rename(columns={\"index\": \"column\"})\n",
    "    return rep.sort_values(\"%missing\", ascending=False)\n",
    "\n",
    "ultra = build_ultra_report(df)\n",
    "ts = timestamp()\n",
    "csv_path   = ARTIFACTS / f\"{ts}_ultra_report_raw_merged.csv\"\n",
    "excel_path = ARTIFACTS / f\"{ts}_ultra_report_raw_merged.xlsx\"\n",
    "\n",
    "ultra.to_csv(csv_path, index=False)\n",
    "try:\n",
    "    ultra.to_excel(excel_path, index=False)\n",
    "    print(f\"[Saved] Ultra Report → {csv_path.name} and {excel_path.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"[Saved] Ultra Report → {csv_path.name} (Excel export skipped: {e})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6ebbe4-ac8b-4547-8a10-8e2814273366",
   "metadata": {
    "id": "bc6ebbe4-ac8b-4547-8a10-8e2814273366",
    "outputId": "22bf9b7e-1c3a-4778-e946-79998c51237e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate pairs by _x suffix: ['workclass_x', 'education_x', 'marital.status_x', 'occupation_x', 'relationship_x']\n",
      "=== *_x/_y consolidation report ===\n",
      "             base                          action  rows_total  \\\n",
      "0       education  conflicts_flagged_keep_sources        5646   \n",
      "1      occupation  conflicts_flagged_keep_sources        5646   \n",
      "2    relationship  conflicts_flagged_keep_sources        5646   \n",
      "3       workclass  conflicts_flagged_keep_sources        5646   \n",
      "4  marital.status        identical_merged_drop_xy        5646   \n",
      "5          TOTALS                                        5646   \n",
      "\n",
      "   n_fill_x_from_y  n_fill_y_from_x  n_conflicts  \n",
      "0                0                0         3475  \n",
      "1                0              274         4710  \n",
      "2                0                0         1859  \n",
      "3                0              274         1689  \n",
      "4                0                0            0  \n",
      "5                0              548        11733  \n",
      "[Saved] artifacts\\20250914-172944_post_structural_clean.csv + 20250914-172944_post_structural_clean_schema.json\n",
      "=== After Structural Cleaning ===\n",
      "\n",
      "Rows: 5,646, Columns: 28\n",
      "\n",
      "--- Dtypes summary ---\n",
      "object     17\n",
      "int64       7\n",
      "boolean     4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Unique values (Top 10) ---\n",
      "ID                 5646\n",
      "fnlwgt             5004\n",
      "capital.loss         92\n",
      "hours.per.week       76\n",
      "age                  69\n",
      "education_file2      16\n",
      "education.num        16\n",
      "education_file1      16\n",
      "education            16\n",
      "occupation           15\n",
      "dtype: int64\n",
      "\n",
      "--- Missing values (Top 10) ---\n",
      "workclass_file2     274\n",
      "occupation_file2    274\n",
      "native.country      114\n",
      "dtype: int64\n",
      "\n",
      "--- Duplicated rows ---\n",
      "Total duplicated rows: 0\n",
      "\n",
      "--- Descriptive statistics (numeric, Top 10) ---\n",
      "                 count          mean            std        min         25%  \\\n",
      "age             5646.0  4.194793e+01      13.326337       17.0       32.00   \n",
      "fnlwgt          5646.0  1.880733e+05  103473.567902    19302.0   117478.75   \n",
      "education.num   5646.0  1.076922e+01       2.654484        1.0        9.00   \n",
      "ID              5646.0  1.551757e+06    1640.265137  1548921.0  1550334.25   \n",
      "hours.per.week  5646.0  4.251931e+01      12.286565        1.0       40.00   \n",
      "sex             5646.0  2.582359e-01       0.437703        0.0        0.00   \n",
      "capital.loss    5646.0  5.028277e+02     852.307416        0.0        0.00   \n",
      "\n",
      "                      50%         75%        max  \n",
      "age                  41.0       51.00       90.0  \n",
      "fnlwgt           176255.0   237311.00  1033222.0  \n",
      "education.num        10.0       13.00       16.0  \n",
      "ID              1551754.5  1553178.75  1554597.0  \n",
      "hours.per.week       40.0       50.00       99.0  \n",
      "sex                   0.0        1.00        1.0  \n",
      "capital.loss          0.0     1485.00     4356.0  \n",
      "\n",
      "--- Descriptive statistics (categorical/text, Top 10) ---\n",
      "                   count unique             top  freq\n",
      "workclass_file1     5646      8         Private  3683\n",
      "education_file1     5646     16         HS-grad  1573\n",
      "occupation_file1    5646     15  Prof-specialty  1019\n",
      "relationship_file1  5646      6         Husband  2981\n",
      "workclass_file2     5372      7         Private  3683\n",
      "education_file2     5646     16          HSgrad  1573\n",
      "occupation_file2    5372     14   Profspecialty  1019\n",
      "native.country      5532     13    UnitedStates  5126\n",
      "income              5646      2           <=50K  2921\n",
      "relationship_file2  5646      6         Husband  2981\n"
     ]
    }
   ],
   "source": [
    "# === STEP 8) STRUCTURAL CLEANING — Resolve *_x / *_y duplicate pairs =========\n",
    "# Purpose:\n",
    "# 1) Detect and consolidate column pairs created by merge (e.g., 'education_x' & 'education_y').\n",
    "# 2) Clear rules per pair:\n",
    "#    - If ALL values are identical → keep <base>, drop _x/_y.\n",
    "#    - If differences are only NaN vs non-NaN → fill from the other side, drop _x/_y.\n",
    "#    - If true conflicts (both non-NaN and different) → create unified <base> (combine_first),\n",
    "#      retain sources as <base>_file1 / <base>_file2, and add boolean <base>_conflict flag.\n",
    "# 3) Produce a compact report with counts: how many values were filled, how many conflicts remain.\n",
    "#\n",
    "# Why now?\n",
    "# After merge, duplicated columns (_x/_y) clutter the schema and may disagree.\n",
    "# Cleaning structure first prevents type-conversion headaches later.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# --- (A) Quick preview: which *_x pairs exist (and do they have a matching *_y)? ---\n",
    "x_cols_preview = [c for c in df.columns if c.endswith('_x')]\n",
    "print(\"Candidate pairs by _x suffix:\", x_cols_preview or \"None\")\n",
    "\n",
    "# --- (B) Helper: robust string view for safe comparisons across dtypes ----------\n",
    "def _as_string(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Rationale: Compare heterogenous columns safely (object/category/number).\n",
    "    Action: Cast to pandas 'string' dtype and normalize obvious empties.\n",
    "    \"\"\"\n",
    "    if pd.api.types.is_string_dtype(s) or s.dtype == \"object\" or pd.api.types.is_categorical_dtype(s):\n",
    "        s = s.astype(\"string\")\n",
    "    else:\n",
    "        # For numeric/datetime/boolean, cast to string but preserve NaNs\n",
    "        s = s.astype(\"string\")\n",
    "    # Normalize trivial empties\n",
    "    return s.replace({\"\": pd.NA, \"nan\": pd.NA, \"None\": pd.NA})\n",
    "\n",
    "# --- (C) Core function: resolve all *_x/*_y pairs according to the policy -----\n",
    "def resolve_xy_duplicates(df_in: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_in : DataFrame with potential *_x/*_y duplicate columns.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_out : DataFrame after consolidation.\n",
    "    report : Per-base-column summary (action taken, counts filled, conflicts).\n",
    "    \"\"\"\n",
    "    df_out = df_in.copy()\n",
    "    rows = []\n",
    "\n",
    "    # Find all bases that have both _x and _y\n",
    "    x_cols = [c for c in df_out.columns if c.endswith('_x')]\n",
    "    for x_col in x_cols:\n",
    "        base = x_col[:-2]  # remove \"_x\"\n",
    "        y_col = base + \"_y\"\n",
    "        if y_col not in df_out.columns:\n",
    "            continue  # not a pair → skip\n",
    "\n",
    "        # Compare as normalized strings (robust to category mismatch)\n",
    "        sx = _as_string(df_out[x_col])\n",
    "        sy = _as_string(df_out[y_col])\n",
    "\n",
    "        # Equal if same or both NA (element-wise)\n",
    "        eq_mask = (sx == sy) | (sx.isna() & sy.isna())\n",
    "        all_equal = bool(eq_mask.all())\n",
    "\n",
    "        # Fillable where x is NA and y is not; similarly for the reverse\n",
    "        fill_x_from_y = df_out[x_col].isna() & df_out[y_col].notna()\n",
    "        fill_y_from_x = df_out[y_col].isna() & df_out[x_col].notna()\n",
    "        n_fill_x_from_y = int(fill_x_from_y.sum())\n",
    "        n_fill_y_from_x = int(fill_y_from_x.sum())\n",
    "\n",
    "        # True conflicts: both non-NA and different\n",
    "        conflict_mask = df_out[x_col].notna() & df_out[y_col].notna() & ~(sx == sy)\n",
    "        n_conflicts = int(conflict_mask.sum())\n",
    "\n",
    "        if all_equal:\n",
    "            # Action: collapse to a single <base>\n",
    "            df_out[base] = df_out[x_col]\n",
    "            df_out.drop(columns=[x_col, y_col], inplace=True)\n",
    "            action = \"identical_merged_drop_xy\"\n",
    "\n",
    "        elif n_conflicts == 0:\n",
    "            # Action: only NA differences → combine_first is enough; drop _x/_y\n",
    "            df_out[base] = df_out[x_col].combine_first(df_out[y_col])\n",
    "            df_out.drop(columns=[x_col, y_col], inplace=True)\n",
    "            action = \"filled_nans_drop_xy\"\n",
    "\n",
    "        else:\n",
    "            # Action: conflicts exist → keep unified base + retain sources + flag conflicts\n",
    "            df_out[base] = df_out[x_col].combine_first(df_out[y_col])\n",
    "            df_out[base + \"_conflict\"] = conflict_mask\n",
    "            df_out.rename(columns={x_col: base + \"_file1\", y_col: base + \"_file2\"}, inplace=True)\n",
    "            action = \"conflicts_flagged_keep_sources\"\n",
    "\n",
    "        rows.append({\n",
    "            \"base\": base,\n",
    "            \"action\": action,\n",
    "            \"rows_total\": len(df_out),\n",
    "            \"n_fill_x_from_y\": n_fill_x_from_y,\n",
    "            \"n_fill_y_from_x\": n_fill_y_from_x,\n",
    "            \"n_conflicts\": n_conflicts\n",
    "        })\n",
    "\n",
    "    report = pd.DataFrame(rows).sort_values([\"action\", \"base\"]) if rows else pd.DataFrame(columns=[\n",
    "        \"base\",\"action\",\"rows_total\",\"n_fill_x_from_y\",\"n_fill_y_from_x\",\"n_conflicts\"\n",
    "    ])\n",
    "\n",
    "    # Totals row for quick at-a-glance\n",
    "    if not report.empty:\n",
    "        totals = {\n",
    "            \"base\": \"TOTALS\",\n",
    "            \"action\": \"\",\n",
    "            \"rows_total\": report[\"rows_total\"].max(),\n",
    "            \"n_fill_x_from_y\": int(report[\"n_fill_x_from_y\"].sum()),\n",
    "            \"n_fill_y_from_x\": int(report[\"n_fill_y_from_x\"].sum()),\n",
    "            \"n_conflicts\": int(report[\"n_conflicts\"].sum())\n",
    "        }\n",
    "        report = pd.concat([report, pd.DataFrame([totals])], ignore_index=True)\n",
    "\n",
    "    return df_out, report\n",
    "\n",
    "# --- (D) Run the consolidation on the current working df ---------------------\n",
    "df_before_struct = df.copy()  # keep for your own reference (optional)\n",
    "\n",
    "df, xy_report = resolve_xy_duplicates(df)\n",
    "\n",
    "print(\"=== *_x/_y consolidation report ===\")\n",
    "print(xy_report if not xy_report.empty else \"No _x/_y pairs found.\")\n",
    "\n",
    "# --- (E) (Optional) Quick post-cleaning snapshot/report ----------------------\n",
    "# Save a snapshot so you can always reload this milestone.\n",
    "save_snapshot_df(df, \"post_structural_clean\")\n",
    "\n",
    "# Brief health check after structural cleaning (re-uses your quick_data_report)\n",
    "quick_data_report(df, title=\"After Structural Cleaning\", top_n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5773c1a-d177-4772-8a13-dc574d27c7fd",
   "metadata": {
    "id": "e5773c1a-d177-4772-8a13-dc574d27c7fd",
    "outputId": "04b4b4b1-5836-4f4c-fa98-a971901b512d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Soft-conflict resolution summary ===\n",
      "           base  conflicts_before  resolved_by_normalization  \\\n",
      "1     education              3475                       3475   \n",
      "2    occupation              4710                       4710   \n",
      "3  relationship              1859                       1859   \n",
      "0     workclass              1689                       1689   \n",
      "\n",
      "   conflicts_remaining  dropped_sources  \n",
      "1                    0             True  \n",
      "2                    0             True  \n",
      "3                    0             True  \n",
      "0                    0             True  \n",
      "[Saved] artifacts\\20250914-203008_post_soft_conflict_normalization.csv + 20250914-203008_post_soft_conflict_normalization_schema.json\n",
      "=== After Soft Conflict Normalization ===\n",
      "\n",
      "Rows: 5,646, Columns: 16\n",
      "\n",
      "--- Dtypes summary ---\n",
      "object    9\n",
      "int64     7\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Unique values (Top 10) ---\n",
      "ID                5646\n",
      "fnlwgt            5004\n",
      "capital.loss        92\n",
      "hours.per.week      76\n",
      "age                 69\n",
      "education.num       16\n",
      "education           16\n",
      "occupation          15\n",
      "native.country      14\n",
      "workclass            8\n",
      "dtype: int64\n",
      "\n",
      "--- Missing values (Top 10) ---\n",
      "native.country    114\n",
      "dtype: int64\n",
      "\n",
      "--- Duplicated rows ---\n",
      "Total duplicated rows: 0\n",
      "\n",
      "--- Descriptive statistics (numeric, Top 10) ---\n",
      "                 count          mean            std        min         25%  \\\n",
      "age             5646.0  4.194793e+01      13.326337       17.0       32.00   \n",
      "fnlwgt          5646.0  1.880733e+05  103473.567902    19302.0   117478.75   \n",
      "education.num   5646.0  1.076922e+01       2.654484        1.0        9.00   \n",
      "ID              5646.0  1.551757e+06    1640.265137  1548921.0  1550334.25   \n",
      "hours.per.week  5646.0  4.251931e+01      12.286565        1.0       40.00   \n",
      "sex             5646.0  2.582359e-01       0.437703        0.0        0.00   \n",
      "capital.loss    5646.0  5.028277e+02     852.307416        0.0        0.00   \n",
      "\n",
      "                      50%         75%        max  \n",
      "age                  41.0       51.00       90.0  \n",
      "fnlwgt           176255.0   237311.00  1033222.0  \n",
      "education.num        10.0       13.00       16.0  \n",
      "ID              1551754.5  1553178.75  1554597.0  \n",
      "hours.per.week       40.0       50.00       99.0  \n",
      "sex                   0.0        1.00        1.0  \n",
      "capital.loss          0.0     1485.00     4356.0  \n",
      "\n",
      "--- Descriptive statistics (categorical/text, Top 10) ---\n",
      "               count unique                 top  freq\n",
      "native.country  5532     13        UnitedStates  5126\n",
      "income          5646      2               <=50K  2921\n",
      "race            5646      5               White  4937\n",
      "age_range       5646      6               45-69  2212\n",
      "workclass       5646      8             Private  3683\n",
      "education       5646     16             HS-grad  1573\n",
      "marital.status  5646      7  Married-civ-spouse  3388\n",
      "occupation      5646     15      Prof-specialty  1019\n",
      "relationship    5646      6             Husband  2981\n"
     ]
    }
   ],
   "source": [
    "# === STEP 9) NORMALIZE & RESOLVE \"SOFT\" CONFLICTS BETWEEN _file1/_file2 =====\n",
    "# Purpose:\n",
    "# 1) For bases that still have <base>_file1 / <base>_file2 and <base>_conflict,\n",
    "#    try to resolve disagreements caused only by formatting (spaces/dashes/case).\n",
    "# 2) If normalized values match → choose a canonical value (by default, file1),\n",
    "#    clear the conflict flag, and (if no conflicts remain) drop the source columns.\n",
    "# 3) Produce a compact report of before/after conflict counts and actions taken.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# --- Helper: normalization for category-like strings -------------------------\n",
    "def normalize_token(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Rationale: Many differences are superficial (e.g., 'HS-grad' vs 'HSgrad').\n",
    "    Action: Lowercase; remove spaces, hyphens, underscores; strip.\n",
    "    \"\"\"\n",
    "    s = s.astype(\"string\")\n",
    "    s = s.str.strip().str.lower()\n",
    "    s = s.str.replace(r\"[\\s\\-_]+\", \"\", regex=True)\n",
    "    # Treat empty-like as NA\n",
    "    return s.replace({\"\": pd.NA, \"nan\": pd.NA, \"none\": pd.NA})\n",
    "\n",
    "# --- Core function: resolve soft conflicts for all bases with *_conflict -----\n",
    "def resolve_soft_conflicts(df_in: pd.DataFrame, prefer: str = \"file1\"):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_in : DataFrame expected to contain columns:\n",
    "            <base> (unified), <base>_file1, <base>_file2, <base>_conflict (boolean)\n",
    "    prefer : Which side to keep as canonical when normalization shows equivalence.\n",
    "             Options: \"file1\" or \"file2\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_out : DataFrame after normalization-based reconciliation.\n",
    "    report : Per-base summary with before/after conflict counts and actions.\n",
    "    pair_tables : dict(base -> DataFrame) of top unresolved (file1,file2) pairs (for manual review).\n",
    "    \"\"\"\n",
    "    assert prefer in {\"file1\", \"file2\"}\n",
    "    df_out = df_in.copy()\n",
    "    rows = []\n",
    "    pair_tables = {}\n",
    "\n",
    "    # Identify all bases that currently have a conflict flag\n",
    "    bases = [c[:-9] for c in df_out.columns if c.endswith(\"_conflict\")]\n",
    "\n",
    "    for base in bases:\n",
    "        f1, f2, flag = f\"{base}_file1\", f\"{base}_file2\", f\"{base}_conflict\"\n",
    "        if not ({f1, f2, flag} <= set(df_out.columns)):\n",
    "            # Not a kept-sources base (maybe already resolved earlier)\n",
    "            continue\n",
    "\n",
    "        # Initial conflict count (before)\n",
    "        before_conf = int(df_out[flag].sum())\n",
    "\n",
    "        # Normalize both sides for safe comparison\n",
    "        n1 = normalize_token(df_out[f1])\n",
    "        n2 = normalize_token(df_out[f2])\n",
    "\n",
    "        # Rows where normalized values match (i.e., only formatting differed)\n",
    "        same_norm = n1.notna() & (n1 == n2)\n",
    "\n",
    "        # Apply canonical choice on those rows\n",
    "        if prefer == \"file1\":\n",
    "            df_out.loc[same_norm, base] = df_out.loc[same_norm, f1]\n",
    "        else:\n",
    "            df_out.loc[same_norm, base] = df_out.loc[same_norm, f2]\n",
    "\n",
    "        # Recompute conflict flag: true conflicts remain where both non-NA and normalized differ\n",
    "        still_conflict = (\n",
    "            df_out[f1].notna() & df_out[f2].notna() &\n",
    "            (normalize_token(df_out[f1]) != normalize_token(df_out[f2]))\n",
    "        )\n",
    "        df_out[flag] = still_conflict\n",
    "\n",
    "        # Build a small “disagreement table” for remaining conflicts (top 20)\n",
    "        unresolved_mask = df_out[flag].fillna(False)\n",
    "        if unresolved_mask.any():\n",
    "            pairs = (\n",
    "                df_out.loc[unresolved_mask, [f1, f2]]\n",
    "                      .value_counts(dropna=False)\n",
    "                      .reset_index(name=\"count\")\n",
    "                      .sort_values(\"count\", ascending=False)\n",
    "                      .head(20)\n",
    "            )\n",
    "            pair_tables[base] = pairs\n",
    "        else:\n",
    "            pair_tables[base] = pd.DataFrame(columns=[f1, f2, \"count\"])\n",
    "\n",
    "        after_conf = int(df_out[flag].sum())\n",
    "        resolved_by_norm = before_conf - after_conf\n",
    "\n",
    "        # If no conflicts left → safe to drop the source columns and the flag\n",
    "        dropped_sources = False\n",
    "        if after_conf == 0:\n",
    "            df_out.drop(columns=[f1, f2, flag], inplace=True, errors=\"ignore\")\n",
    "            dropped_sources = True\n",
    "\n",
    "        rows.append({\n",
    "            \"base\": base,\n",
    "            \"conflicts_before\": before_conf,\n",
    "            \"resolved_by_normalization\": resolved_by_norm,\n",
    "            \"conflicts_remaining\": after_conf,\n",
    "            \"dropped_sources\": dropped_sources\n",
    "        })\n",
    "\n",
    "    report = pd.DataFrame(rows).sort_values([\"conflicts_remaining\", \"base\"])\n",
    "    return df_out, report, pair_tables\n",
    "\n",
    "# --- Run soft-conflict resolution on the current df --------------------------\n",
    "df_before_soft = df.copy()\n",
    "\n",
    "df, soft_report, pair_tables = resolve_soft_conflicts(df, prefer=\"file1\")\n",
    "\n",
    "print(\"=== Soft-conflict resolution summary ===\")\n",
    "print(soft_report if not soft_report.empty else \"No conflict flags were present.\")\n",
    "\n",
    "# Optional: inspect top unresolved value-pairs per base (if any remain)\n",
    "for base, tbl in pair_tables.items():\n",
    "    if not tbl.empty:\n",
    "        print(f\"\\nTop unresolved (file1 vs file2) pairs for '{base}':\")\n",
    "        print(tbl)\n",
    "\n",
    "# Save a snapshot at this milestone and a quick health check\n",
    "save_snapshot_df(df, \"post_soft_conflict_normalization\")\n",
    "quick_data_report(df, title=\"After Soft Conflict Normalization\", top_n=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fcab9c-d33c-4e46-8bd9-de2b94a986c6",
   "metadata": {
    "id": "c7fcab9c-d33c-4e46-8bd9-de2b94a986c6"
   },
   "source": [
    "### 3. Convert an objects into viable data\n",
    "(string, int, float....)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f32b62-5490-44ab-9b58-b6e5f5d7bc50",
   "metadata": {
    "id": "63f32b62-5490-44ab-9b58-b6e5f5d7bc50"
   },
   "source": [
    "###STEP 10) TEXT STANDARDIZATION + TYPE CONVERSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43d25f3-f46d-4b26-a185-43168de44974",
   "metadata": {
    "id": "a43d25f3-f46d-4b26-a185-43168de44974",
    "outputId": "be557843-4756-40d7-8ffa-f894fe6a47dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dtype counts after conversions:\n",
      " int64             6\n",
      "string[python]    1\n",
      "category          1\n",
      "category          1\n",
      "category          1\n",
      "category          1\n",
      "category          1\n",
      "category          1\n",
      "category          1\n",
      "category          1\n",
      "category          1\n",
      "Name: count, dtype: int64\n",
      "[Saved] artifacts\\20250914-203828_post_type_conversions.csv + 20250914-203828_post_type_conversions_schema.json\n",
      "=== After Type Conversions ===\n",
      "\n",
      "Rows: 5,646, Columns: 16\n",
      "\n",
      "--- Dtypes summary ---\n",
      "int64             6\n",
      "string[python]    1\n",
      "category          1\n",
      "category          1\n",
      "category          1\n",
      "category          1\n",
      "category          1\n",
      "category          1\n",
      "category          1\n",
      "category          1\n",
      "category          1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- Unique values (Top 10) ---\n",
      "ID                5646\n",
      "fnlwgt            5004\n",
      "capital.loss        92\n",
      "hours.per.week      76\n",
      "age                 69\n",
      "education.num       16\n",
      "education           16\n",
      "occupation          15\n",
      "native.country      14\n",
      "workclass            8\n",
      "dtype: int64\n",
      "\n",
      "--- Missing values (Top 10) ---\n",
      "native.country    114\n",
      "dtype: int64\n",
      "\n",
      "--- Duplicated rows ---\n",
      "Total duplicated rows: 0\n",
      "\n",
      "--- Descriptive statistics (numeric, Top 10) ---\n",
      "                 count           mean            std      min        25%  \\\n",
      "age             5646.0      41.947928      13.326337     17.0      32.00   \n",
      "fnlwgt          5646.0  188073.334750  103473.567902  19302.0  117478.75   \n",
      "education.num   5646.0      10.769217       2.654484      1.0       9.00   \n",
      "hours.per.week  5646.0      42.519306      12.286565      1.0      40.00   \n",
      "sex             5646.0       0.258236       0.437703      0.0       0.00   \n",
      "capital.loss    5646.0     502.827666     852.307416      0.0       0.00   \n",
      "\n",
      "                     50%       75%        max  \n",
      "age                 41.0      51.0       90.0  \n",
      "fnlwgt          176255.0  237311.0  1033222.0  \n",
      "education.num       10.0      13.0       16.0  \n",
      "hours.per.week      40.0      50.0       99.0  \n",
      "sex                  0.0       1.0        1.0  \n",
      "capital.loss         0.0    1485.0     4356.0  \n",
      "\n",
      "--- Descriptive statistics (categorical/text, Top 10) ---\n",
      "               count unique                 top  freq\n",
      "ID              5646   5646             1548921     1\n",
      "native.country  5532     13        UnitedStates  5126\n",
      "income          5646      2               <=50K  2921\n",
      "race            5646      5               White  4937\n",
      "age_range       5646      6               45-69  2212\n",
      "workclass       5646      8             Private  3683\n",
      "education       5646     16             HS-grad  1573\n",
      "marital.status  5646      7  Married-civ-spouse  3388\n",
      "occupation      5646     15      Prof-specialty  1019\n",
      "relationship    5646      6             Husband  2981\n"
     ]
    }
   ],
   "source": [
    "# Goal:\n",
    "# 1) Normalize textual values across all object-like columns (trim/case/whitespace).\n",
    "# 2) Convert columns from generic 'object' to the most suitable dtype:\n",
    "#    - IDs / free-text → string\n",
    "#    - Low-cardinality categories → category (optionally ordered)\n",
    "#    - Numeric-in-text → numeric (if any)\n",
    "#    - Dates-in-text → datetime (if any)\n",
    "# Notes:\n",
    "# We won't impute missing values here; handle that in the dedicated \"Missing Data\" step.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- (A) Helper: standardize strings for a list of columns -------------------\n",
    "# Rationale: reduce accidental category proliferation (e.g., trailing spaces/case).\n",
    "def standardize_text_columns(df: pd.DataFrame, cols: list[str] | None = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Normalize common nuisances in textual/categorical columns:\n",
    "      - strip leading/trailing whitespace\n",
    "      - collapse internal whitespace runs to a single space\n",
    "      - unify case (title/lower as you prefer; we keep original tokens except trimming)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    if cols is None:\n",
    "        cols = df.select_dtypes(include=['object','string','category']).columns.tolist()\n",
    "\n",
    "    for c in cols:\n",
    "        s = df[c].astype('string')  # safe view\n",
    "        # trim outside\n",
    "        s = s.str.strip()\n",
    "        # collapse internal whitespace (e.g. \"New   York\" -> \"New York\")\n",
    "        s = s.str.replace(r\"\\s+\", \" \", regex=True)\n",
    "        # normalize empty-like tokens\n",
    "        s = s.replace({\"\": pd.NA, \"nan\": pd.NA, \"None\": pd.NA})\n",
    "        df[c] = s\n",
    "    return df\n",
    "\n",
    "# --- (B) Decide target types for your dataset --------------------------------\n",
    "# Based on your current columns after soft-conflict step:\n",
    "# candidates for 'category'\n",
    "cat_candidates = [\n",
    "    'workclass','education','marital.status','occupation',\n",
    "    'relationship','race','native.country','age_range','income'\n",
    "]\n",
    "# IDs / identifiers → string (not a model feature)\n",
    "id_candidates = ['ID']\n",
    "\n",
    "# Ordered categories example (age_range)\n",
    "age_range_order = ['<18','18-24','25-34','35-44','45-69','>70']\n",
    "\n",
    "# --- (C) Apply text standardization ------------------------------------------\n",
    "df = standardize_text_columns(df)  # normalize all object-like columns\n",
    "\n",
    "# --- (D) Convert IDs to string explicitly -----------------------------------\n",
    "for col in id_candidates:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype('string')\n",
    "\n",
    "# --- (E) Convert selected columns to category (unordered/ordered) ------------\n",
    "for col in cat_candidates:\n",
    "    if col in df.columns:\n",
    "        if col == 'age_range':\n",
    "            # Make age_range an ordered categorical\n",
    "            present = [lvl for lvl in age_range_order if lvl in set(df[col].dropna().unique())]\n",
    "            df[col] = pd.Categorical(df[col], categories=age_range_order, ordered=True)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "# --- (F) Optional: detect numeric-in-text and convert (none obvious here) ----\n",
    "# Example pattern if needed in future:\n",
    "# numeric_like = ['some_numeric_text_col']\n",
    "# for col in numeric_like:\n",
    "#     if col in df.columns:\n",
    "#         df[col] = pd.to_numeric(df[col].astype('string').str.replace(\",\",\"\"), errors='coerce')\n",
    "\n",
    "# --- (G) Optional: detect date-in-text and convert (none obvious here) -------\n",
    "# Example:\n",
    "# date_like = ['some_date_col']\n",
    "# for col in date_like:\n",
    "#     if col in df.columns:\n",
    "#         df[col] = pd.to_datetime(df[col].astype('string'), errors='coerce', dayfirst=False)\n",
    "\n",
    "# --- (H) Sanity check + save snapshot ---------------------------------------\n",
    "print(\"Dtype counts after conversions:\\n\", df.dtypes.value_counts())\n",
    "save_snapshot_df(df, \"post_type_conversions\")\n",
    "quick_data_report(df, title=\"After Type Conversions\", top_n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1602ec1-676e-44e0-8a57-34465a9b8c22",
   "metadata": {
    "id": "c1602ec1-676e-44e0-8a57-34465a9b8c22",
    "outputId": "04751d77-470a-465f-f527-e7ce3120aeb5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5646 entries, 0 to 5645\n",
      "Data columns (total 16 columns):\n",
      " #   Column          Non-Null Count  Dtype   \n",
      "---  ------          --------------  -----   \n",
      " 0   age             5646 non-null   int64   \n",
      " 1   fnlwgt          5646 non-null   int64   \n",
      " 2   education.num   5646 non-null   int64   \n",
      " 3   ID              5646 non-null   string  \n",
      " 4   hours.per.week  5646 non-null   int64   \n",
      " 5   native.country  5532 non-null   category\n",
      " 6   income          5646 non-null   category\n",
      " 7   race            5646 non-null   category\n",
      " 8   sex             5646 non-null   int64   \n",
      " 9   capital.loss    5646 non-null   int64   \n",
      " 10  age_range       5646 non-null   category\n",
      " 11  workclass       5646 non-null   category\n",
      " 12  education       5646 non-null   category\n",
      " 13  marital.status  5646 non-null   category\n",
      " 14  occupation      5646 non-null   category\n",
      " 15  relationship    5646 non-null   category\n",
      "dtypes: category(9), int64(6), string(1)\n",
      "memory usage: 362.0 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e3f975-4e8f-426a-afeb-6fde917ce8e7",
   "metadata": {
    "id": "15e3f975-4e8f-426a-afeb-6fde917ce8e7",
    "outputId": "323632d0-2939-4f55-8f4f-de059daa02fc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>fnlwgt</th>\n",
       "      <th>education.num</th>\n",
       "      <th>hours.per.week</th>\n",
       "      <th>sex</th>\n",
       "      <th>capital.loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5646.000000</td>\n",
       "      <td>5.646000e+03</td>\n",
       "      <td>5646.000000</td>\n",
       "      <td>5646.000000</td>\n",
       "      <td>5646.000000</td>\n",
       "      <td>5646.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>41.947928</td>\n",
       "      <td>1.880733e+05</td>\n",
       "      <td>10.769217</td>\n",
       "      <td>42.519306</td>\n",
       "      <td>0.258236</td>\n",
       "      <td>502.827666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.326337</td>\n",
       "      <td>1.034736e+05</td>\n",
       "      <td>2.654484</td>\n",
       "      <td>12.286565</td>\n",
       "      <td>0.437703</td>\n",
       "      <td>852.307416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.000000</td>\n",
       "      <td>1.930200e+04</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>32.000000</td>\n",
       "      <td>1.174788e+05</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>41.000000</td>\n",
       "      <td>1.762550e+05</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>51.000000</td>\n",
       "      <td>2.373110e+05</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1485.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90.000000</td>\n",
       "      <td>1.033222e+06</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4356.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               age        fnlwgt  education.num  hours.per.week          sex  \\\n",
       "count  5646.000000  5.646000e+03    5646.000000     5646.000000  5646.000000   \n",
       "mean     41.947928  1.880733e+05      10.769217       42.519306     0.258236   \n",
       "std      13.326337  1.034736e+05       2.654484       12.286565     0.437703   \n",
       "min      17.000000  1.930200e+04       1.000000        1.000000     0.000000   \n",
       "25%      32.000000  1.174788e+05       9.000000       40.000000     0.000000   \n",
       "50%      41.000000  1.762550e+05      10.000000       40.000000     0.000000   \n",
       "75%      51.000000  2.373110e+05      13.000000       50.000000     1.000000   \n",
       "max      90.000000  1.033222e+06      16.000000       99.000000     1.000000   \n",
       "\n",
       "       capital.loss  \n",
       "count   5646.000000  \n",
       "mean     502.827666  \n",
       "std      852.307416  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.000000  \n",
       "75%     1485.000000  \n",
       "max     4356.000000  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()   # סטטיסטיקות בסיסיות"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e5356d-9116-499a-bd8c-1a8434bfd828",
   "metadata": {
    "id": "59e5356d-9116-499a-bd8c-1a8434bfd828",
    "outputId": "1c04d199-771f-4238-acec-ff747856e970"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                 0\n",
       "fnlwgt              0\n",
       "education.num       0\n",
       "ID                  0\n",
       "hours.per.week      0\n",
       "native.country    114\n",
       "income              0\n",
       "race                0\n",
       "sex                 0\n",
       "capital.loss        0\n",
       "age_range           0\n",
       "workclass           0\n",
       "education           0\n",
       "marital.status      0\n",
       "occupation          0\n",
       "relationship        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum() # ספירת חסרים"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27dec79-67cb-4eff-a80c-d1ea78e329fe",
   "metadata": {
    "id": "e27dec79-67cb-4eff-a80c-d1ea78e329fe",
    "outputId": "5fda5ea4-dd1b-457a-dc34-b90d16d57d1a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum() # בדיקת כפילויות שורות"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607a0aa7-ed48-45a3-8ee5-70c31abb4163",
   "metadata": {
    "id": "607a0aa7-ed48-45a3-8ee5-70c31abb4163"
   },
   "outputs": [],
   "source": [
    "# We can replace df['income'] values to avoid risk of changing the values in other columns\n",
    "df=df.replace(to_replace=\"<=50K\", value=\"0\")\n",
    "df=df.replace(to_replace=\">50K\", value=\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0599f39-f2d6-4cb1-9504-0c6946bc4722",
   "metadata": {
    "id": "c0599f39-f2d6-4cb1-9504-0c6946bc4722",
    "outputId": "f2690d1c-e123-4c51-9721-e745c46039c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "income\n",
       "0    2921\n",
       "1    2725\n",
       "Name: count, dtype: Int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['income'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1fed03-0a4e-4f22-9845-423957424076",
   "metadata": {
    "id": "4e1fed03-0a4e-4f22-9845-423957424076"
   },
   "source": [
    "# Values of the categorial feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646349c9-ff76-4fee-b993-db3b6f765623",
   "metadata": {
    "id": "646349c9-ff76-4fee-b993-db3b6f765623",
    "outputId": "8c8c9bd7-58c7-44ac-af33-df7490e72584"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "education\n",
       "HS-grad         1573\n",
       "Bachelors       1185\n",
       "Some-college    1082\n",
       "Masters          486\n",
       "Assoc-voc        256\n",
       "Prof-school      230\n",
       "Assoc-acdm       192\n",
       "11th             144\n",
       "Doctorate        141\n",
       "10th             109\n",
       "7th-8th           84\n",
       "9th               56\n",
       "12th              44\n",
       "5th-6th           39\n",
       "1st-4th           19\n",
       "Preschool          6\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Values of the categorial feature\n",
    "df['education'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992cc3b2-69ca-43b4-9354-23bd186ccdeb",
   "metadata": {
    "id": "992cc3b2-69ca-43b4-9354-23bd186ccdeb"
   },
   "outputs": [],
   "source": [
    "df['education']=df['education'].astype('string')\n",
    "df['education']=df['education'].str.replace('-','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6ad4da-e8f0-4651-8116-c1a900390131",
   "metadata": {
    "id": "5b6ad4da-e8f0-4651-8116-c1a900390131"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee18e19-1ed6-4eda-81ec-7ca581df0091",
   "metadata": {
    "id": "bee18e19-1ed6-4eda-81ec-7ca581df0091"
   },
   "source": [
    "### 5. Clean Text - remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4de18ef-8fe9-4516-82d0-927b7bef52c3",
   "metadata": {
    "id": "c4de18ef-8fe9-4516-82d0-927b7bef52c3"
   },
   "outputs": [],
   "source": [
    "#!pip install string\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42cf9a3-c525-4290-95dd-0d5807bda21b",
   "metadata": {
    "id": "c42cf9a3-c525-4290-95dd-0d5807bda21b"
   },
   "outputs": [],
   "source": [
    "# Columns to convert\n",
    "columns_to_convert = ['workclass', 'education', 'relationship', 'sex', 'race','occupation','native.country']\n",
    "\n",
    "# Convert specified columns to string type and create string_df\n",
    "string_df = df[columns_to_convert].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07522568-f36c-4046-876a-625767dfe380",
   "metadata": {
    "id": "07522568-f36c-4046-876a-625767dfe380"
   },
   "outputs": [],
   "source": [
    "# Convert specified columns to string type and create string_df\n",
    "string_df = df[columns_to_convert].astype(str)\n",
    "\n",
    "# Remove punctuation in one line\n",
    "string_df = string_df.applymap(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "\n",
    "# Assign the cleaned columns back to the original DataFrame\n",
    "df[columns_to_convert] = string_df\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c459c7-b82b-4d6f-a60e-2f49ca2b9a59",
   "metadata": {
    "id": "d3c459c7-b82b-4d6f-a60e-2f49ca2b9a59"
   },
   "source": [
    "exploring features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e69694-0a33-427f-b730-986bc95c3102",
   "metadata": {
    "id": "b0e69694-0a33-427f-b730-986bc95c3102"
   },
   "outputs": [],
   "source": [
    "df['race'].value_counts()\n",
    "#df['workclass'].value_counts()\n",
    "#df['relationship'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f9b1d8-ccd5-4095-ac84-e3f0750e2a38",
   "metadata": {
    "id": "70f9b1d8-ccd5-4095-ac84-e3f0750e2a38"
   },
   "source": [
    "### 6. Narrowing Categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af96507-a9d9-4f61-a202-ba671f858c1e",
   "metadata": {
    "id": "0af96507-a9d9-4f61-a202-ba671f858c1e"
   },
   "source": [
    "Exploriring the 'native.country' category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263c7c84-a9ff-42cb-b878-cde19c46c004",
   "metadata": {
    "id": "263c7c84-a9ff-42cb-b878-cde19c46c004",
    "outputId": "0fb89310-7b60-41c1-cc55-0969758c2d0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "native.country\n",
       "UnitedStates             5126\n",
       "Asia                      155\n",
       "North America             109\n",
       "Europe                     64\n",
       "UK                         21\n",
       "ElSalvador                 14\n",
       "PuertoRico                 14\n",
       "South America              12\n",
       "Center America              7\n",
       "DominicanRepublic           6\n",
       "TrinadadTobago              2\n",
       "OutlyingUSGuamUSVIetc       1\n",
       "HolandNetherlands           1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['native.country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0a9e0a-97f5-4f97-b43c-3f158ae61fb7",
   "metadata": {
    "id": "ec0a9e0a-97f5-4f97-b43c-3f158ae61fb7",
    "outputId": "3a6d45ae-5045-430a-b3d2-b6de4e6f8492",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Country→Region mapping summary ===\n",
      "Region value counts:\n",
      "native.region\n",
      "North America      5256\n",
      "Asia                155\n",
      "Unknown             114\n",
      "Europe               86\n",
      "Central America      21\n",
      "South America        12\n",
      "Caribbean             2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unmapped raw country tokens (review needed):\n",
      "native.country\n",
      "Asia                     0\n",
      "Center America           0\n",
      "DominicanRepublic        0\n",
      "ElSalvador               0\n",
      "Europe                   0\n",
      "HolandNetherlands        0\n",
      "North America            0\n",
      "OutlyingUSGuamUSVIetc    0\n",
      "PuertoRico               0\n",
      "South America            0\n",
      "TrinadadTobago           0\n",
      "UK                       0\n",
      "UnitedStates             0\n",
      "Name: count, dtype: int64\n",
      "[Saved] artifacts\\20250914-213935_post_region_mapping_tailored.csv + 20250914-213935_post_region_mapping_tailored_schema.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'artifacts\\\\20250914-213935_post_region_mapping_tailored'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === COUNTRY → REGION (tailored to your unique values) =======================\n",
    "# Purpose:\n",
    "# - Map 'native.country' to a new regional feature 'native.region'.\n",
    "# - Do NOT overwrite the original country column.\n",
    "# - Handle existing region labels, typos, and NaNs safely.\n",
    "# - Print a coverage report (what got mapped; anything left unmapped).\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "col = \"native.country\"\n",
    "assert col in df.columns, \"'native.country' column is missing.\"\n",
    "\n",
    "# (A) Normalizer for robust matching (lowercase; remove spaces/punct)\n",
    "def _norm_token(s: pd.Series) -> pd.Series:\n",
    "    s = s.astype(\"string\")\n",
    "    s = s.str.strip().str.lower()\n",
    "    s = s.str.replace(r\"[^\\w\\s]\", \"\", regex=True).str.replace(r\"\\s+\", \"\", regex=True)\n",
    "    return s.replace({\"\": pd.NA, \"nan\": pd.NA, \"none\": pd.NA})\n",
    "\n",
    "country_norm = _norm_token(df[col])\n",
    "\n",
    "# (B) Canonical region names we accept “as-is” if they already appear\n",
    "ALREADY_REGION = {\n",
    "    \"North America\", \"South America\", \"Central America\",\n",
    "    \"Caribbean\", \"Europe\", \"Asia\", \"Africa\", \"Oceania\", \"Middle East\"\n",
    "}\n",
    "\n",
    "# (C) Normalized dictionary for your observed tokens\n",
    "MAP_NORM = {\n",
    "    # Regions already in the data (keep as canonical)\n",
    "    \"northamerica\": \"North America\",\n",
    "    \"southamerica\": \"South America\",\n",
    "    \"centeramerica\": \"Central America\",  # your data uses \"Center America\"\n",
    "    \"centralamerica\": \"Central America\",\n",
    "    \"europe\": \"Europe\",\n",
    "    \"asia\": \"Asia\",\n",
    "\n",
    "    # United States / territories\n",
    "    \"unitedstates\": \"North America\",\n",
    "    \"outlyingusguamusvietc\": \"North America\",\n",
    "\n",
    "    # UK and Netherlands → Europe\n",
    "    \"uk\": \"Europe\",\n",
    "    \"holandnetherlands\": \"Europe\",\n",
    "\n",
    "    # Specific countries seen in your data\n",
    "    \"elsalvador\": \"Central America\",\n",
    "    \"dominicanrepublic\": \"North America\",  # follow supervisor's choice\n",
    "    \"puertorico\": \"North America\",         # follow supervisor's choice\n",
    "    \"trinadadtobago\": \"Caribbean\",         # typo variant; choose Caribbean by default\n",
    "}\n",
    "\n",
    "# (D) Row-wise mapping function (preserves existing region labels)\n",
    "def map_to_region(raw_val, norm_val):\n",
    "    # Missing → Unknown\n",
    "    if pd.isna(raw_val):\n",
    "        return \"Unknown\"\n",
    "    # If the raw string is already a region label we accept, keep it title-cased\n",
    "    if isinstance(raw_val, str) and raw_val.strip().title() in ALREADY_REGION:\n",
    "        return raw_val.strip().title()\n",
    "    # Else map via normalized dictionary\n",
    "    if pd.notna(norm_val) and norm_val in MAP_NORM:\n",
    "        return MAP_NORM[norm_val]\n",
    "    # Fallback\n",
    "    return \"Other\"\n",
    "\n",
    "df[\"native.region\"] = [\n",
    "    map_to_region(r, n) for r, n in zip(df[col].tolist(), country_norm.tolist())\n",
    "]\n",
    "df[\"native.region\"] = df[\"native.region\"].astype(\"category\")\n",
    "\n",
    "# (E) Coverage report\n",
    "print(\"=== Country→Region mapping summary ===\")\n",
    "print(\"Region value counts:\")\n",
    "print(df[\"native.region\"].value_counts(dropna=False))\n",
    "\n",
    "# Any raw tokens not covered by the dictionary and not already regions?\n",
    "unmapped_mask = (~country_norm.isna()) & (~country_norm.isin(MAP_NORM.keys()))\n",
    "unmapped_raw = (\n",
    "    df.loc[unmapped_mask & ~df[col].isin(ALREADY_REGION), col]\n",
    "      .dropna()\n",
    "      .value_counts()\n",
    ")\n",
    "if not unmapped_raw.empty:\n",
    "    print(\"\\nUnmapped raw country tokens (review needed):\")\n",
    "    print(unmapped_raw)\n",
    "else:\n",
    "    print(\"\\nAll observed tokens were mapped or recognized as regions.\")\n",
    "\n",
    "# (F) Optional: snapshot\n",
    "save_snapshot_df(df, \"post_region_mapping_tailored\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603347a3-50d3-4932-84e9-d87d1d579cd6",
   "metadata": {
    "id": "603347a3-50d3-4932-84e9-d87d1d579cd6",
    "outputId": "2ff90b9f-6e73-4a80-c5cc-59805a048bc0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Changed rows: 0\n"
     ]
    }
   ],
   "source": [
    "before = df['native.country'].value_counts(dropna=False)\n",
    "# ... run your mapping ...\n",
    "after = df['native.country'].value_counts(dropna=False)\n",
    "print(\"Changed rows:\", int((before != after).sum() > 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53578c06-9ac7-4580-af6d-00373b6616bb",
   "metadata": {
    "id": "53578c06-9ac7-4580-af6d-00373b6616bb"
   },
   "source": [
    "## Changing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872e5eee-3e33-4978-a19b-d9e02705e7f5",
   "metadata": {
    "id": "872e5eee-3e33-4978-a19b-d9e02705e7f5"
   },
   "outputs": [],
   "source": [
    "# --- Ensure 'sex' is clean binary and keep a readable label ------------------\n",
    "# Rationale: enforce numeric binary type; keep a label column for readability.\n",
    "if 'sex' in df.columns:\n",
    "    # Coerce to Int8/boolean, assuming values already 0/1\n",
    "    df['sex'] = df['sex'].astype('Int8')  # or: df['sex'] = df['sex'].astype('boolean')\n",
    "    # Optional: keep a human-readable copy\n",
    "    sex_map = {0: 'Male', 1: 'Female'}\n",
    "    df['sex_label'] = df['sex'].map(sex_map).astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5393b62-dc59-49b8-baf2-614b76ebf438",
   "metadata": {
    "id": "c5393b62-dc59-49b8-baf2-614b76ebf438",
    "outputId": "8b4cc5e7-d478-454f-bd83-70c570254e47"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sex\n",
       "0    4188\n",
       "1    1458\n",
       "Name: count, dtype: Int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sex'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf605355-06a5-49f2-a1ae-4d79f4dd9722",
   "metadata": {
    "id": "bf605355-06a5-49f2-a1ae-4d79f4dd9722"
   },
   "source": [
    "#### Task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0691128b-5bdf-46a8-b932-3a0958cb969d",
   "metadata": {
    "id": "0691128b-5bdf-46a8-b932-3a0958cb969d"
   },
   "source": [
    "1-Create age groups <br> 2- Crate a countplot for the groups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff248be-b8a7-44f8-921d-338deb507873",
   "metadata": {
    "id": "eff248be-b8a7-44f8-921d-338deb507873"
   },
   "source": [
    "### Age Groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89d7666-2364-4f86-8290-3fbbc61e212b",
   "metadata": {
    "id": "a89d7666-2364-4f86-8290-3fbbc61e212b",
    "outputId": "b35ed367-fed7-4207-b870-f2bd22868b6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age_range\n",
      "45-69    2212\n",
      "35-44    1650\n",
      "25-34    1162\n",
      "18-24     387\n",
      ">70       191\n",
      "<18        44\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- Recompute age_range from numeric 'age' with explicit, ordered bins ------\n",
    "# Rationale: left-closed, right-open intervals; ordered categorical for modeling/EDA.\n",
    "bins   = [0, 18, 24, 34, 44, 69, float('inf')]\n",
    "labels = ['<18', '18-24', '25-34', '35-44', '45-69', '>70']\n",
    "\n",
    "df['age_range'] = pd.cut(df['age'], bins=bins, labels=labels, right=False, include_lowest=True)\n",
    "df['age_range'] = pd.Categorical(df['age_range'], categories=labels, ordered=True)\n",
    "\n",
    "# Quick check\n",
    "print(df['age_range'].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98db2fca-e652-4ee2-9720-533be32ea009",
   "metadata": {
    "id": "98db2fca-e652-4ee2-9720-533be32ea009",
    "outputId": "f1c9af6e-0c5e-419a-ca26-5b0adc8ba2f9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age_range</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;18</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18-24</td>\n",
       "      <td>387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25-34</td>\n",
       "      <td>1162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35-44</td>\n",
       "      <td>1650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45-69</td>\n",
       "      <td>2212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&gt;70</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  age_range  count\n",
       "0       <18     44\n",
       "1     18-24    387\n",
       "2     25-34   1162\n",
       "3     35-44   1650\n",
       "4     45-69   2212\n",
       "5       >70    191"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aggregate the age groups\n",
    "age_group_aggregation = df.groupby('age_range').size().reset_index(name='count')\n",
    "\n",
    "age_group_aggregation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46623935-7c71-4d55-aa65-5881cff4b965",
   "metadata": {
    "id": "46623935-7c71-4d55-aa65-5881cff4b965",
    "outputId": "90004d89-0862-418b-b2f5-fcc2ee1196a1"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a countplot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='age_range', data=df, order=['<18', '18-24', '25-34', '35-44', '45-69', '>70'])\n",
    "plt.title('Count of Age Groups')\n",
    "plt.xlabel('Age Range')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175d291a-1ff0-436b-b7bc-890c51c21aaf",
   "metadata": {
    "id": "175d291a-1ff0-436b-b7bc-890c51c21aaf",
    "outputId": "69c15464-f717-4bf9-ed94-66f00d24bb56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "education_group\n",
      "UpperSecondary       1870\n",
      "SomeCollege/Assoc    1530\n",
      "Bachelors            1185\n",
      "Graduate              857\n",
      "LowerSecondary        140\n",
      "Primary                64\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# --- Map 'education' to coarse groups (keeps original; creates 'education_group') ---\n",
    "# Rationale: domain-level grouping improves stability and interpretability.\n",
    "\n",
    "def _norm_token_series(s: pd.Series) -> pd.Series:\n",
    "    s = s.astype('string').str.strip().str.lower()\n",
    "    s = s.str.replace(r\"[^\\w\\s]\", \"\", regex=True).str.replace(r\"\\s+\", \"\", regex=True)\n",
    "    return s.replace({\"\": pd.NA, \"nan\": pd.NA, \"none\": pd.NA})\n",
    "\n",
    "# Build normalized token → group dictionary (adult-income style taxonomy)\n",
    "edu_map_norm = {\n",
    "    # Primary / Elementary\n",
    "    \"preschool\": \"Primary\",\n",
    "    \"1st4th\": \"Primary\",\n",
    "    \"5th6th\": \"Primary\",\n",
    "    \"7th8th\": \"LowerSecondary\",\n",
    "    \"9th\": \"LowerSecondary\",\n",
    "    \"10th\": \"UpperSecondary\",\n",
    "    \"11th\": \"UpperSecondary\",\n",
    "    \"12th\": \"UpperSecondary\",\n",
    "\n",
    "    # High school graduate\n",
    "    \"hsgrad\": \"UpperSecondary\",       # covers HS-grad / HSgrad\n",
    "\n",
    "    # Some college / Associate\n",
    "    \"somecollege\": \"SomeCollege/Assoc\",\n",
    "    \"assocacdm\": \"SomeCollege/Assoc\",\n",
    "    \"assocvoc\": \"SomeCollege/Assoc\",\n",
    "\n",
    "    # Higher education\n",
    "    \"bachelors\": \"Bachelors\",\n",
    "    \"masters\": \"Graduate\",\n",
    "    \"profschool\": \"Graduate\",\n",
    "    \"doctorate\": \"Graduate\",\n",
    "}\n",
    "\n",
    "if 'education' in df.columns:\n",
    "    edu_norm = _norm_token_series(df['education'])\n",
    "    df['education_group'] = edu_norm.map(edu_map_norm).fillna('Other').astype('category')\n",
    "\n",
    "# Peek\n",
    "print(df['education_group'].value_counts(dropna=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a617b4-910a-4cf4-a50b-60447bf8682b",
   "metadata": {
    "id": "97a617b4-910a-4cf4-a50b-60447bf8682b",
    "outputId": "2071ae4d-04d0-44ec-bd9a-555b93e64538"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             column  n_unique  missing  %missing  \\\n",
      "1         education        16        0      0.00   \n",
      "6        occupation        15        0      0.00   \n",
      "4    native.country        14      114      2.02   \n",
      "10        workclass         8        0      0.00   \n",
      "3    marital.status         7        0      0.00   \n",
      "5     native.region         7        0      0.00   \n",
      "8      relationship         6        0      0.00   \n",
      "2   education_group         6        0      0.00   \n",
      "0         age_range         6        0      0.00   \n",
      "7              race         5        0      0.00   \n",
      "9         sex_label         2        0      0.00   \n",
      "\n",
      "                                           top_levels  rare_share  \n",
      "1   HS-grad:1573; Bachelors:1185; Some-college:108...       18.85  \n",
      "6   Prof-specialty:1019; Exec-managerial:996; Craf...       31.28  \n",
      "4   UnitedStates:5126; Asia:155; North America:109...        1.01  \n",
      "10  Private:3683; Self-emp-not-inc:525; Local-gov:...        7.35  \n",
      "3   Married-civ-spouse:3388; Never-married:1263; D...        0.97  \n",
      "5   North America:5256; Asia:155; Unknown:114; Eur...        0.25  \n",
      "8   Husband:2981; Not-in-family:1277; Own-child:48...        1.77  \n",
      "2   UpperSecondary:1870; SomeCollege/Assoc:1530; B...        1.13  \n",
      "0   45-69:2212; 35-44:1650; 25-34:1162; 18-24:387;...        0.78  \n",
      "7   White:4937; Black:457; AsianPacIslander:178; A...        0.00  \n",
      "9                              Male:4188; Female:1458        0.00  \n",
      "Columns selected for lumping (rule-of-thumb): ['education', 'occupation', 'native.country', 'workclass']\n"
     ]
    }
   ],
   "source": [
    "# --- Categorical audit + optional rare-level lumping (generic) ---------------\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "def categorical_audit(df, exclude=None, top=5):\n",
    "    \"\"\"\n",
    "    Build a per-column audit for object/string/category columns:\n",
    "    - n_unique, missing, %missing\n",
    "    - top-k levels with counts (as a compact string)\n",
    "    - rare_share (proportion outside top-k)\n",
    "    \"\"\"\n",
    "    if exclude is None:\n",
    "        exclude = []\n",
    "    cols = df.select_dtypes(include=['object','string','category']).columns.difference(exclude)\n",
    "    rows = []\n",
    "    n = len(df)\n",
    "    for c in cols:\n",
    "        vc = df[c].value_counts(dropna=True)\n",
    "        miss = int(df[c].isna().sum())\n",
    "        topk = \"; \".join([f\"{idx}:{cnt}\" for idx, cnt in vc.head(top).items()])\n",
    "        rare_share = float(vc.iloc[top:].sum())/n if len(vc) > top else 0.0\n",
    "        rows.append({\n",
    "            \"column\": c,\n",
    "            \"n_unique\": int(df[c].nunique(dropna=False)),\n",
    "            \"missing\": miss,\n",
    "            \"%missing\": round(miss*100/n, 2),\n",
    "            \"top_levels\": topk,\n",
    "            \"rare_share\": round(rare_share*100, 2),\n",
    "        })\n",
    "    return pd.DataFrame(rows).sort_values([\"n_unique\",\"rare_share\"], ascending=[False, False])\n",
    "\n",
    "audit = categorical_audit(df, exclude=['income','ID'])  # exclude target/ID from audit\n",
    "print(audit.head(12))\n",
    "\n",
    "# Optional: apply rare→Other to selected columns\n",
    "def lump_rare(df_in, cols, min_prop=0.01, label=\"Other\"):\n",
    "    df_out = df_in.copy()\n",
    "    n = len(df_out)\n",
    "    min_count = max(1, int(np.ceil(min_prop * n)))\n",
    "    summary = []\n",
    "    for c in cols:\n",
    "        vc = df_out[c].value_counts(dropna=True)\n",
    "        keep = vc[vc >= min_count].index\n",
    "        # ensure label doesn't collide\n",
    "        lab = label if label not in keep else f\"{label}_~\"\n",
    "        before = int(df_out[c].nunique(dropna=False))\n",
    "        df_out[c] = df_out[c].where(df_out[c].isin(keep), other=lab).astype('category')\n",
    "        after = int(df_out[c].nunique(dropna=False))\n",
    "        summary.append({\"column\": c, \"levels_before\": before, \"levels_after\": after,\n",
    "                        \"min_count\": min_count, \"other_label\": lab,\n",
    "                        \"%rows_other\": round((df_out[c].eq(lab).mean()*100),2)})\n",
    "    return df_out, pd.DataFrame(summary).sort_values(\"levels_before\", ascending=False)\n",
    "\n",
    "# Example: lump only columns whose audit suggests high rare_share or high n_unique:\n",
    "to_lump = audit.query(\"n_unique > 10 or rare_share > 5\")['column'].tolist()\n",
    "print(\"Columns selected for lumping (rule-of-thumb):\", to_lump)\n",
    "\n",
    "# If you want to actually lump:\n",
    "# df, lump_report = lump_rare(df, to_lump, min_prop=0.01, label=\"Other\")\n",
    "# print(lump_report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99ea38d-8963-4da8-911a-9a652c60070f",
   "metadata": {
    "id": "d99ea38d-8963-4da8-911a-9a652c60070f",
    "outputId": "e2d8adf6-70b3-46fb-f029-3422fd91377f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "native.country\n",
       "UnitedStates             5126\n",
       "Asia                      155\n",
       "North America             109\n",
       "Europe                     64\n",
       "UK                         21\n",
       "ElSalvador                 14\n",
       "PuertoRico                 14\n",
       "South America              12\n",
       "Center America              7\n",
       "DominicanRepublic           6\n",
       "TrinadadTobago              2\n",
       "OutlyingUSGuamUSVIetc       1\n",
       "HolandNetherlands           1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['native.country'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad258325-368b-40cf-a4cf-6d92c565c9bb",
   "metadata": {
    "id": "ad258325-368b-40cf-a4cf-6d92c565c9bb",
    "outputId": "17f7c009-0a2d-4a2e-bffd-f065b089cb51"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "education\n",
       "HS-grad         1573\n",
       "Bachelors       1185\n",
       "Some-college    1082\n",
       "Masters          486\n",
       "Assoc-voc        256\n",
       "Prof-school      230\n",
       "Assoc-acdm       192\n",
       "11th             144\n",
       "Doctorate        141\n",
       "10th             109\n",
       "7th-8th           84\n",
       "9th               56\n",
       "12th              44\n",
       "5th-6th           39\n",
       "1st-4th           19\n",
       "Preschool          6\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['education'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b211b6-65dd-4449-bd7b-8f5d2a140693",
   "metadata": {
    "id": "b6b211b6-65dd-4449-bd7b-8f5d2a140693",
    "outputId": "c0e93479-99aa-4ef2-fc16-a1bb4c859892"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "occupation\n",
       "Prof-specialty       1019\n",
       "Exec-managerial       996\n",
       "Craft-repair          676\n",
       "Sales                 662\n",
       "Adm-clerical          527\n",
       "Other-service         344\n",
       "Machine-op-inspct     287\n",
       "?                     274\n",
       "Transport-moving      259\n",
       "Tech-support          184\n",
       "Handlers-cleaners     154\n",
       "Farming-fishing       142\n",
       "Protective-serv       105\n",
       "Priv-house-serv        14\n",
       "Armed-Forces            3\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['occupation'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4fce6f-e472-42ff-bbed-e197a59d61c0",
   "metadata": {
    "id": "fd4fce6f-e472-42ff-bbed-e197a59d61c0"
   },
   "source": [
    "## Saving Manipulated data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479311ed-072a-493e-9df9-d1290d0a3b6c",
   "metadata": {
    "id": "479311ed-072a-493e-9df9-d1290d0a3b6c",
    "outputId": "fbbc6b42-dc00-4a1e-e365-37f00ad4bf8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved] artifacts\\20250914-220536_final_for_eda.csv + 20250914-220536_final_for_eda_schema.json\n",
      "Saved at: artifacts\\20250914-220536_final_for_eda\n"
     ]
    }
   ],
   "source": [
    "FINAL_BASE = save_snapshot_df(df, \"final_for_eda\")\n",
    "print(\"Saved at:\", FINAL_BASE)\n",
    "# יווצרו קבצים תחת artifacts/: parquet/או csv + קובץ schema.json עם dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec1b009-4584-4784-ad19-aae7c18a21cd",
   "metadata": {
    "id": "bec1b009-4584-4784-ad19-aae7c18a21cd"
   },
   "source": [
    "Another option is Pickle file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5782e2-ab8d-41d9-a19b-1ec5bc636d0d",
   "metadata": {
    "id": "1b5782e2-ab8d-41d9-a19b-1ec5bc636d0d"
   },
   "source": [
    "# Pickle file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756a0771-bb24-47b9-997f-d1fd935fdde7",
   "metadata": {
    "id": "756a0771-bb24-47b9-997f-d1fd935fdde7"
   },
   "source": [
    "A pickle file in Python is used to serialize and deserialize Python objects. <br>Serialization (write binary) is the process of converting a Python object into a byte stream, which can then be written to a file or transmitted over a network. <br>Deserialization (read binary) is the reverse process, converting the byte stream back into a Python object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f5ee31-6e18-407f-a222-67475fc09001",
   "metadata": {
    "id": "c7f5ee31-6e18-407f-a222-67475fc09001"
   },
   "outputs": [],
   "source": [
    "df.to_pickle('mrg_df_after_data_prep_5.2.25.pkl')\n",
    "df=pd.read_pickle('mrg_df_after_data_prep_5.2.25.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33d6abf-a6b9-480d-a452-d9fea18e3d0d",
   "metadata": {
    "id": "f33d6abf-a6b9-480d-a452-d9fea18e3d0d",
    "outputId": "9c7286f9-0b62-46c0-b82a-5fcfbeecc286"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "income dataset saved as a pickle file.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "#Write a pickle file\n",
    "with open('income.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)\n",
    "\n",
    "print(\"income dataset saved as a pickle file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9985c70-6ad4-42a3-aaaf-6e9aa31baaba",
   "metadata": {
    "id": "b9985c70-6ad4-42a3-aaaf-6e9aa31baaba"
   },
   "outputs": [],
   "source": [
    "#Read a pickle file and load the data\n",
    "with open('income.pkl', 'rb') as file:\n",
    "   loaded_data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d9d9f6-ef1d-4936-9a38-6172a9fd63d9",
   "metadata": {
    "id": "56d9d9f6-ef1d-4936-9a38-6172a9fd63d9"
   },
   "outputs": [],
   "source": [
    "loaded_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0c6fde-5f93-47f2-b230-d4e2c0914557",
   "metadata": {
    "id": "ef0c6fde-5f93-47f2-b230-d4e2c0914557"
   },
   "source": [
    "# We got A flat file, Next to EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dc144f-9b2e-4f00-ab61-d6cc331a3fbd",
   "metadata": {
    "id": "14dc144f-9b2e-4f00-ab61-d6cc331a3fbd"
   },
   "source": [
    "# DATA PREP — Step‑by‑Step Summary (2025‑09‑14)\n",
    "\n",
    "Goal: produce a single, clean, merged dataset ready for a separate EDA notebook and future modeling. This log lists what we did, why, and what we saved.\n",
    "\n",
    "## Step 1 — Load inputs\n",
    "\n",
    "Load two raw files as df1 and df2.\n",
    "Work in Jupyter; keep all outputs under artifacts/.\n",
    "\n",
    "## Step 2 — Quick health reports\n",
    "\n",
    "Run quick_data_report(df, title, top_n) on each file to see shape, dtypes, missing values, duplicates, and quick stats.\n",
    "Build an Ultra Report (one row per column: dtype, missing, unique, sample values) and save CSV/Excel.\n",
    "Saved: artifacts/<timestamp>_ultra_report_*.csv|.xlsx\n",
    "\n",
    "## Step 3 — Schema comparison\n",
    "\n",
    "Compare columns and dtypes across df1 and df2.\n",
    "Identify: common columns, columns only in each file, and any dtype mismatches (none in our case).\n",
    "\n",
    "## Step 4 — Merge‑key checks\n",
    "\n",
    "Test candidates (e.g., fnlwgt, age+fnlwgt, fnlwgt+education.num, age+education.num+fnlwgt).\n",
    "For each: check missingness, duplicates by key, and key overlap.\n",
    "Decision: use ['age','workclass','fnlwgt'] (after text standardization) for an inner join.\n",
    "\n",
    "## Step 5 — Merge\n",
    "\n",
    "Merge to merged_df = pd.merge(df1, df2, on=['age','workclass','fnlwgt'], how='inner').\n",
    "Result: 5,646 rows × 21 cols.\n",
    "Save a raw snapshot of the merged table.\n",
    "Saved: artifacts/<timestamp>_raw_loaded.*\n",
    "\n",
    "## Step 6 — Structural cleaning (post‑merge)\n",
    "\n",
    "Detect _x/_y column pairs (workclass/education/marital.status/occupation/relationship).\n",
    "Consolidation rules per pair:\n",
    "If values are equal → keep one column.\n",
    "If one side is NA → fill from the other.\n",
    "If conflicting → keep both sources for now and flag.\n",
    "\n",
    "## Step 7 — Soft normalization of text\n",
    "\n",
    "Standardize textual categories (trim spaces, collapse whitespace, remove literal hyphens) to collapse near‑duplicates (e.g., HS-grad vs HSgrad).\n",
    "After normalization, many “conflicts” disappeared and _x/_y sources were dropped safely.\n",
    "Resulting table size after cleanup: 5,646 × 16 (no duplicates; tidy categories).\n",
    "\n",
    "## Step 8 — Type setup\n",
    "\n",
    "Standardize all string‑like columns (preserve <NA>).\n",
    "Convert key categoricals to category dtype (memory‑efficient): workclass, education, marital.status, occupation, relationship, race, native.country, age_range.\n",
    "Ensure age_range is ordered: <18, 18–24, 25–34, 35–44, 45–69, >70.\n",
    "Ensure identifiers like ID are string.\n",
    "\n",
    "Target:\n",
    "Keep income_label (readable),\n",
    "Map income → binary {<=50K:0, >50K:1} as Int8.\n",
    "\n",
    "## Step 9 — Missing data\n",
    "\n",
    "Only native.country had missing values (114 rows ≈ 2.0%).\n",
    "Create flag native.country_was_missing; fill to explicit category \"Unknown\".\n",
    "Save snapshot.\n",
    "\n",
    "## Step 10 — Domain mappings\n",
    "\n",
    "Country → Region: create native.region using a normalized dictionary; keep native.country unchanged.\n",
    "Regions: North America, Europe, Asia, Central America, South America, Caribbean, Unknown.\n",
    "Education grouping: create education_group with interpretable buckets: Primary, LowerSecondary, UpperSecondary, SomeCollege/Assoc, Bachelors, Graduate.\n",
    "Save snapshot.\n",
    "\n",
    "## Step 11 — Categorical audit (coverage & “tail”)\n",
    "\n",
    "Build an audit table for all categorical features: n_unique, %missing, top levels, rare_share.\n",
    "Guidance:\n",
    "Rule‑of‑thumb: levels with <~1% of rows are “rare” and candidates for Other (only when helpful).\n",
    "Prefer domain groupings (education_group, native.region) over blind Other.\n",
    "\n",
    "## Step 12 — Decisions on rare levels\n",
    "\n",
    "education: do not lump; use education_group for modeling/EDA and keep education as reference.\n",
    "native.country: do not lump; use native.region; keep countries as reference.\n",
    "occupation: replace '?' → Unknown. Optionally create occupation_lumped at ~1% (keep original intact).\n",
    "workclass: optional lumping at ~0.5–1%.\n",
    "\n",
    "## Step 13 — Age bands\n",
    "\n",
    "Verify/compute age_range from numeric age with explicit bins and ordered categories (left‑closed, right‑open).\n",
    "Step 14 — Final saves for EDA\n",
    "Preferred: Parquet (if pyarrow/fastparquet available) + CSV (portable) + PKL (Python‑only cache).\n",
    "If Parquet engine missing now, keep CSV/PKL; install later and re‑export.\n",
    "df.to_parquet('artifacts/final_for_eda.parquet', index=False)\n",
    "df.to_csv('artifacts/final_for_eda.csv', index=False, encoding='utf-8')\n",
    "df.to_pickle('artifacts/final_for_eda.pkl')\n",
    "\n",
    "## Step 15 — What to use in the EDA notebook\n",
    "\n",
    "Use income (0/1) as target and keep income_label for readable plots.\n",
    "Prefer education_group and native.region (stable, low‑cardinality) over raw education/native.country.\n",
    "Consider occupation_lumped if created; otherwise use occupation with a clear Unknown level.\n",
    "Use age_range (ordered), sex (0/1 with sex_label), and other cleaned categoricals.\n",
    "\n",
    "## Artifacts we saved along the way (examples)\n",
    "\n",
    "artifacts/<ts>_raw_loaded.* — raw merged snapshot (+ schema JSON)\n",
    "artifacts/<ts>_ultra_report_raw_merged.csv|.xlsx — column profile\n",
    "artifacts/<ts>_post_structural_clean.* — after _x/_y consolidation\n",
    "artifacts/<ts>_post_soft_conflict_normalization.* — after text normalization\n",
    "artifacts/<ts>_post_missing_handling.* — after handling native.country missings\n",
    "artifacts/<ts>_post_region_mapping_tailored.* — after region mapping\n",
    "(optional) artifacts/<ts>_post_lumping_occ_workclass.* — if rare‑level lumping applied\n",
    "artifacts/final_for_eda.(parquet|csv|pkl) — clean dataset for the next EDA notebook\n",
    "\n",
    "Next: open a fresh EDA notebook; load final_for_eda and start descriptive analysis (frequency tables, cross‑tabs with income, and plots).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0839c9c-c44d-403c-a582-3075a9deec8b",
   "metadata": {
    "id": "f0839c9c-c44d-403c-a582-3075a9deec8b"
   },
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09425ffc-2d60-49fa-9100-73bbbd199b69",
   "metadata": {
    "id": "09425ffc-2d60-49fa-9100-73bbbd199b69"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"Beer_data_ EDA/income_v2.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a4c3a2-77dd-4609-8659-77052cbf32b4",
   "metadata": {
    "id": "64a4c3a2-77dd-4609-8659-77052cbf32b4"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80bab25-8756-4950-8f6b-f771818e5fda",
   "metadata": {
    "id": "a80bab25-8756-4950-8f6b-f771818e5fda"
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
